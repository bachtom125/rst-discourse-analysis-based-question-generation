{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and Preprocess Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29339, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>relation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>converted_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Aesthetic Appreciation and Spanish Art :</td>\n",
       "      <td>Insights from Eye - Tracking</td>\n",
       "      <td>elaboration-additional</td>\n",
       "      <td>13</td>\n",
       "      <td>Elaboration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Claire Bailey - Ross claire.bailey-ross@port.a...</td>\n",
       "      <td>Aesthetic Appreciation and Spanish Art : Insig...</td>\n",
       "      <td>organization-heading</td>\n",
       "      <td>754</td>\n",
       "      <td>Textual-Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Claire Bailey - Ross claire.bailey-ross@port.a...</td>\n",
       "      <td>Andrew Beresford a.m.beresford@durham.ac.uk Du...</td>\n",
       "      <td>joint-list</td>\n",
       "      <td>37</td>\n",
       "      <td>Joint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Andrew Beresford a.m.beresford@durham.ac.uk Du...</td>\n",
       "      <td>Daniel Smith daniel.smith2@durham.ac.uk Durham...</td>\n",
       "      <td>joint-list</td>\n",
       "      <td>26</td>\n",
       "      <td>Joint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Daniel Smith daniel.smith2@durham.ac.uk Durham...</td>\n",
       "      <td>Claire Warwick c.l.h.warwick@durham.ac.uk Durh...</td>\n",
       "      <td>joint-list</td>\n",
       "      <td>18</td>\n",
       "      <td>Joint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_name                                            nucleus  \\\n",
       "0  GUM_academic_art          Aesthetic Appreciation and Spanish Art :    \n",
       "1  GUM_academic_art  Claire Bailey - Ross claire.bailey-ross@port.a...   \n",
       "2  GUM_academic_art  Claire Bailey - Ross claire.bailey-ross@port.a...   \n",
       "3  GUM_academic_art  Andrew Beresford a.m.beresford@durham.ac.uk Du...   \n",
       "4  GUM_academic_art  Daniel Smith daniel.smith2@durham.ac.uk Durham...   \n",
       "\n",
       "                                           satellite                relation  \\\n",
       "0                      Insights from Eye - Tracking   elaboration-additional   \n",
       "1  Aesthetic Appreciation and Spanish Art : Insig...    organization-heading   \n",
       "2  Andrew Beresford a.m.beresford@durham.ac.uk Du...              joint-list   \n",
       "3  Daniel Smith daniel.smith2@durham.ac.uk Durham...              joint-list   \n",
       "4  Claire Warwick c.l.h.warwick@durham.ac.uk Durh...              joint-list   \n",
       "\n",
       "   word_count    converted_relation  \n",
       "0          13           Elaboration  \n",
       "1         754  Textual-Organization  \n",
       "2          37                 Joint  \n",
       "3          26                 Joint  \n",
       "4          18                 Joint  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./total_relations_with_doc_name.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_switch = df.copy() # swith positions of nucleus and satellite\n",
    "df_switch.rename(columns={'nucleus': 'satellite', 'satellite': 'nucleus'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df, df_switch], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required \n",
      "to do this task by hand \n",
      "\n",
      "This system can also be used longitudinally \n",
      "to study how the workforce ’s composition changes over time , \n",
      "\n",
      "and uses the 2011 and 2017 censuses \n",
      "to investigate the “ leaky pipeline ” problem in faculty retention . \n",
      "\n",
      "two additional demographic variables are included . \n",
      "To provide information on the analytical sample as a whole , \n",
      "\n",
      "one must have adequate knowledge of the focal country ’s institutional framework . \n",
      "In order to understand policymaking and its effects , \n",
      "\n",
      "Speakers of Eegimaa use the term Gújjolaay ( meaning Jóola ) \n",
      "to refer to their language \n",
      "\n",
      "but Eegimaa \n",
      "to distinguish their Jóola variety from that of other Jóola peoples . \n",
      "\n",
      "and whether lexical and syntactic criteria are sufficient \n",
      "to identify enjambment . \n",
      "\n",
      "Systematically collecting large amounts of enjambment examples provides helpful evidence \n",
      "to assess scholars ’ current claims , \n",
      "\n",
      "what kinds of knowledge they must utilize \n",
      "to guide their selection of sources \n",
      "\n",
      "The companies hoped to leverage the prestige of Elsevier with these fake journals \n",
      "to endow their promotional “ research ” with an air of reliability . \n",
      "\n",
      "Yet some Frontiers journals appear to have engaged in bad behavior , \n",
      "whether for profit or for some other motive . \n",
      "\n",
      "put in place \n",
      "to track expertise . \n",
      "\n",
      "These institutions do not exist \n",
      "solely to credential experts . \n",
      "\n",
      "and they may try to keep their discoveries in - house \n",
      "to protect their reputations . \n",
      "\n",
      "but we exclude them here \n",
      "to make sure we are comparing like with like . \n",
      "\n",
      "Future studies need to include more participants and more items \n",
      "to increase power . \n",
      "\n",
      "Future studies should also include a larger age range of children \n",
      "to document the age at which adult - like performance emerges . \n",
      "\n",
      "The current study is too small \n",
      "to make strong conclusions with regard to cultural differences and experimental methods ; \n",
      "\n",
      "The cardinal numbers included in this study were only one through five \n",
      "in order to avoid additional item variability , \n",
      "\n",
      "At the initial meeting it became apparent that most participants were there \n",
      "to learn more about digital humanities \n",
      "\n",
      "needed more training \n",
      "to adequately support researchers and students in this field . \n",
      "\n",
      "This UV radiation interacts with the chemicals on the inside of the bulb \n",
      "to generate light . \n",
      "\n",
      "In this article , we propose the definition of specific mutation operators \n",
      "for testing Geographic Information Systems . \n",
      "\n",
      "we rely on Java reflection and aspect - oriented programming . \n",
      "In order to apply these operators to a SUT , \n",
      "\n",
      "By creating a Geofence with an erroneous location from its central location , \n",
      "the device will receive incorrect location notifications . \n",
      "\n",
      "However , these are not adequate \n",
      "to cover errors in particular domains . \n",
      "\n",
      "provides a web infrastructure \n",
      "to facilitate collaboration \n",
      "\n",
      "and a significant improvement has been made \n",
      "to understand the ocean dynamics and climate change . \n",
      "\n",
      "which were developed \n",
      "to estimate the foam covered sea surface emissivity . \n",
      "\n",
      "originally proposed \n",
      "to estimate brightness temperature at higher frequencies , \n",
      "\n",
      "when using simple morphological parameters \n",
      "to estimate functional relationships . \n",
      "\n",
      "Due to the phylogenetic relatedness , extant birds have been used \n",
      "to inform functional aspects of non-avian dinosaur locomotion . \n",
      "\n",
      "who must negotiate an increasingly complex network of paratexts and intertexts \n",
      "in order to fully engage with its narratives . \n",
      "\n",
      "but emigrated \n",
      "to escape the Spanish persecution of the Huguenots . \n",
      "\n",
      "which is designed \n",
      "to create an area above its surface where the air velocity increases . \n",
      "\n",
      "and , one year later , returned to the United States \n",
      "to teach at Harvard . \n",
      "\n",
      "Nearly 30,000 people packed the streets of San Francisco \n",
      "to pay him homage at his funeral . \n",
      "\n",
      "L'Enfant was recruited by Pierre Augustin Caron de Beaumarchais \n",
      "to serve in the American Revolutionary War in the United States . \n",
      "\n",
      "that the author should publish the whole of his work \n",
      "in order to form a definitive opinion . \" \n",
      "\n",
      "I need all my courage \n",
      "to die at twenty ! ) \n",
      "\n",
      "before moving , in 1972 , to London \n",
      "to study at the Architectural Association School of Architecture . \n",
      "\n",
      "Holt moved to left field . \n",
      "in order to allow Xander Bogaerts to play 3rd base , \n",
      "\n",
      "making a sensational catch on the warning track \n",
      "to rob Ian Kinsler of a hit . \n",
      "\n",
      "Jerome initially used classical authors \n",
      "to describe Christian concepts such as hell \n",
      "\n",
      "he took occasion \n",
      "to study the country 's educational system . \n",
      "\n",
      "which we feel instinctively to be adequate \n",
      "to express the ideas they stand for . ” \n",
      "\n",
      "and bringing in people \n",
      "to help , \n",
      "\n",
      "which split words into their components \n",
      "to help determine equivalence in translation \n",
      "\n",
      "to aim at full intelligibility of the reader \n",
      "so he / she may understand the full implications of the message ; \n",
      "\n",
      "Jared decided to move Los Angeles , California instead \n",
      "to pursue an acting career . \n",
      "\n",
      "to release a shirt featuring both of their faces , \n",
      "to benefit their newly formed joint charitable fund . \n",
      "\n",
      "and Theodorus rushed to Tabennese \n",
      "to placate the rebels . \n",
      "\n",
      "So you do n't need to go borrow equipment from anybody , \n",
      "to to do the feet ? Do the hooves ? \n",
      "\n",
      "I 'm gon na wait till like , early in the morning , \n",
      "to do those , \n",
      "\n",
      "Cause I do n't wait till Christmas \n",
      "to buy everything . \n",
      "\n",
      "we 'd wait , \n",
      "to open her gifts and — And our gifts to her . \n",
      "\n",
      "Right , Melanie will call me \n",
      "to confirm your lie . \n",
      "\n",
      "Well you woke me up last night , \n",
      "to tell me Kim was n't spending the night . \n",
      "\n",
      "why you could n't wake me up in the morning , \n",
      "to tell me that you were going . \n",
      "\n",
      "All I need is your signature \n",
      "so I can play the volleyball . On the volleyball team . \n",
      "\n",
      "Do you need a partner ? \n",
      "To go there ? \n",
      "\n",
      "I really ought a call , at least Reg Barr , \n",
      "and ask if uh , he 's still speaking to me . \n",
      "\n",
      "I got big armies , buddy . \n",
      "Trying to conquer the world . \n",
      "\n",
      "and then we can do can squish . And squish em . \n",
      "For the recycling bin . \n",
      "\n",
      "Well , I 'll call him probably late this afternoon , \n",
      "to make sure he 's alright . \n",
      "\n",
      "and take the X - rays , \n",
      "and see if it 's healed , enough to take off . \n",
      "\n",
      "if it 's healed , enough \n",
      "to take off . \n",
      "\n",
      "if you , if you wan na go ahead \n",
      "and do it . \n",
      "\n",
      "I do n't know what I did \n",
      "to get that . \n",
      "\n",
      "And I can always put those back up into the top \n",
      "and , and see if they check . \n",
      "\n",
      "Uh I was called by Matthew here , \n",
      "to do the job for him . \n",
      "\n",
      "to lessen the burden of payment on me , \n",
      "so that I in turn would make more money off the job . \n",
      "\n",
      "So we do n't have to decide everything , \n",
      "to decide something in this case . \n",
      "\n",
      "that Debbie stayed \n",
      "to fight the fire . \n",
      "\n",
      "that she did n't stay \n",
      "to fight the fire \n",
      "\n",
      "she stayed \n",
      "to fight the fire . \n",
      "\n",
      "So Secretary Cardona again invoked the HEROES Act \n",
      "to provide a measure of loan forgiveness \n",
      "\n",
      "To apply the major questions doctrine \n",
      "to override that clear text \n",
      "\n",
      "And here comes Melvin Dobson \n",
      "arriving at the scene to the rescue of his daughter . \n",
      "\n",
      "Knitters leave a bit of yarn sticking out of the day ’s knitting \n",
      "so they know where to pick up the next day \n",
      "\n",
      "that leaps up on your screen \n",
      "to announce something new , \n",
      "\n",
      "While this is n’t enough \n",
      "to live on , \n",
      "\n",
      "We Have to Resolve to the Life We Evolved to Live \n",
      "To Feel Happier , \n",
      "\n",
      "we have to see it in the context where it evolved . \n",
      "to understand how fear works in us , \n",
      "\n",
      "and scavenge \n",
      "to find food ; \n",
      "\n",
      "which required days and miles of hunting and scavenging \n",
      "to acquire , \n",
      "\n",
      "Our ancestors had to be active \n",
      "to survive in the wild . \n",
      "\n",
      "This body did not evolve \n",
      "to sit at a desk eight hours a day , \n",
      "\n",
      "What can we do \n",
      "to feel better ? \n",
      "\n",
      "we should live normal , \n",
      "To feel normal , \n",
      "\n",
      "that is because the human animal had to do so \n",
      "to survive . \n",
      "\n",
      "and help us fight the other tribes \n",
      "to survive . \n",
      "\n",
      "Politicians and the media very often use fear \n",
      "to circumvent our logic . \n",
      "\n",
      "to be abused \n",
      "to turn on our aggression toward “ the others , ” \n",
      "\n",
      "and stood next to the side door , \n",
      "waiting for the rain to stop . \n",
      "\n",
      "Santa bent down \n",
      "to pick them up , \n",
      "\n",
      "She put them in the sink \n",
      "to soak . \n",
      "\n",
      "flying up \n",
      "to rewind their nanosprings in the stratospheric sunlight , \n",
      "\n",
      "flying down \n",
      "to make Frankfurt run . \n",
      "\n",
      "it lets them hack the city ’s person - recognition systems . \n",
      "So the mites do n’t see them when they jump . \n",
      "\n",
      "I say , \n",
      "to distract her \n",
      "\n",
      "She opens the garage door \n",
      "so I can get in . \n",
      "\n",
      "She turns back , \n",
      "watching where she ’s going , \n",
      "\n",
      "and hauling themselves up , \n",
      "to fall backwards into the crest of the oncoming waves . \n",
      "\n",
      "every change of the screen doing nothing \n",
      "to abate it . \n",
      "\n",
      "Chalmers altered his course \n",
      "to attempt to throw off any pursuers . \n",
      "\n",
      "“ I ’m going to give you something \n",
      "to numb it , \n",
      "\n",
      "“ They ’re going to implant a chip . \n",
      "It will help them take care of you . ” \n",
      "\n",
      "and not stopping \n",
      "to pick it up \n",
      "\n",
      "and it does n’t take Freud \n",
      "to figure out what that meant . \n",
      "\n",
      "I watched him raise his arms , almost hesitantly , \n",
      "to ward off the worst of the blows . \n",
      "\n",
      "that appears from nowhere \n",
      "to trip you up en route from A to B . \n",
      "\n",
      "Three decades it took , \n",
      "to see the irony in that remark . \n",
      "\n",
      "Rachel Rook took Carroll home \n",
      "to meet her parents \n",
      "\n",
      "I held a finger up to my lips , and waved a hand \n",
      "to silence Pete ’s reply . \n",
      "\n",
      "“ What if we put something in your nose \n",
      "to catch it , \n",
      "\n",
      "they ’re a sort of collection , and too expensive \n",
      "for me to play with . ” \n",
      "\n",
      "It took a few days good brushing \n",
      "to get it off . \n",
      "\n",
      "that the dentist gave you \n",
      "to rinse your mouth out \n",
      "\n",
      "before poking the setting goo \n",
      "to check its consistency . \n",
      "\n",
      "by using a bright blue putty \n",
      "to build up the form of the jaw , \n",
      "\n",
      "Now , instead of reversing the levers , I had pulled them over \n",
      "so as to go forward with them , \n",
      "\n",
      "It took my tired head a long time \n",
      "to sort that out . \n",
      "\n",
      "People from Earth come here \n",
      "to live . \n",
      "\n",
      "and ran out of the chamber \n",
      "to see what was what . \n",
      "\n",
      "( covered with old CD covers \n",
      "to prevent them escaping ! ) . \n",
      "\n",
      "This gives us the data \n",
      "to do a so - called survival analysis . \n",
      "\n",
      "it takes serious study \n",
      "to become a magician . \n",
      "\n",
      "Troy Patterson of Slate Magazine ventured over to Pacific Standard \n",
      "to sample the new santorum cocktail at the bar . \n",
      "\n",
      "and Savage created a website SpreadingSantorum.com \n",
      "to promulgate the spread of the phenomenon . \n",
      "\n",
      "which is essential \n",
      "to minimize loss of life in these situations . \n",
      "\n",
      "the evacuation is certainly necessary and essential \n",
      "to save lives . \n",
      "\n",
      "that Frank and his colleagues are using \n",
      "to explore the ice \n",
      "\n",
      "The formalin is great \n",
      "for preserving structures , \n",
      "\n",
      "stained the slices \n",
      "to enhance contrast , \n",
      "\n",
      "I used that same compound scope \n",
      "to look at squashed bits of tissue \n",
      "\n",
      "I used that same compound scope to look at squashed bits of tissue \n",
      "to see the stinging capsules ( = nematocysts ) . \n",
      "\n",
      "Wikinews interviewed the site 's founder , Jack Herrick . \n",
      "to discuss what the site has achieved since its creation , \n",
      "\n",
      "like buying carbon offsets \n",
      "to become carbon neutral . \n",
      "\n",
      "This business model worked \n",
      "for producing content on topics \n",
      "\n",
      "However , paying people \n",
      "to write and edit articles \n",
      "\n",
      "So I decided to sell eHow and use the proceeds \n",
      "to build wikiHow . \n",
      "\n",
      "We use a Creative Commons license \n",
      "to give our community the right to fork \n",
      "\n",
      "and editors joined \n",
      "to improve those articles , \n",
      "\n",
      "we must raise money and poll nationally . \n",
      "In order to be invited to the debates , \n",
      "\n",
      "Specifically , altering law once it is passed \n",
      "to accommodate his voting blocs . \n",
      "\n",
      "Ličen has recently joined the Sport Management Program at Washington State University \n",
      "to develop its sport media and communication research and teaching contents . \n",
      "\n",
      "that NBC as the broadcasting rights owner for the United States will use \n",
      "to air the Paralympic Games on . \n",
      "\n",
      "but we will have to wait until after the event \n",
      "to assess to what extent the broadcasters will meet these expectations . \n",
      "\n",
      "using something like the Free Content Definition \n",
      "to limit the scope ? \n",
      "\n",
      "I loaded ferry boats in Jersey City across the river \n",
      "to deliver goods to Ground Zero . \n",
      "\n",
      "you must be willing to go down with the ship \" , \n",
      "\" in order to rise to the occasion \n",
      "\n",
      "Then we assign a headline to a specific writer \n",
      "to execute . \n",
      "\n",
      "That ’s why we are on the 10th floor \n",
      "to make sure they die when they get kicked out . \n",
      "\n",
      "How do you come to an understanding \n",
      "to make peace possible ? \n",
      "\n",
      "So I will mainly talk about how to use a representation theory \n",
      "to solve all kind of problems and conjectures in asymptotic groups theory . \n",
      "\n",
      "It was decided that a collaborative online review process would be used \n",
      "to \" discover \" the microscopically small samples the capsule collected . \n",
      "\n",
      "modified \n",
      "to accelerate dust particles to very high speeds , \n",
      "\n",
      "modified to accelerate dust particles to very high speeds , \n",
      "to simulate the interstellar dust impacts that we 're looking for . \n",
      "\n",
      "and that the Marshall Plan was devised \n",
      "to help countries ravaged by the war , \n",
      "\n",
      "I might do \n",
      "to assist . \n",
      "\n",
      "It is still not too late \n",
      "to turn the tide against the Nationalist - created crisis . \n",
      "\n",
      "of consulting all the people \n",
      "in order to resolve the crisis . \n",
      "\n",
      "for hanging flashlights over our beds \n",
      "for studying \n",
      "\n",
      "He must go slow about selling it to me , \n",
      "waiting on public reaction . \n",
      "\n",
      "The President later personally took action \n",
      "to allow the team into the country . \n",
      "\n",
      "colored blue and orange \n",
      "to represent water and pollution , respectively . \n",
      "\n",
      "leaving them only two weeks left \n",
      "to assemble their robot \n",
      "\n",
      "before packing it up \n",
      "to mail to the competition site . \n",
      "\n",
      "and a warship was deployed \n",
      "to retrieve them . \n",
      "\n",
      "After calling the Scientology organization by phone \n",
      "to attempt to speak with his mother , \n",
      "\n",
      "Rathbun requested donations via a statement made on his blog , \n",
      "in order to help Montalvo with his legal defense financing . \n",
      "\n",
      "a) Leaving a lifehood of slavery \n",
      "to get a taste of freedom and the world . \n",
      "\n",
      "who went out of their ways ( and dipped into their pockets ) \n",
      "to help the kid achieve that freedom . \" \n",
      "\n",
      "Atwood , a Booker Prize - winner , was there \n",
      "to launch her graphic novel Angel Catbird , with illustrator Johnnie Christmas . \n",
      "\n",
      "Volunteer organization Comic Corps of Canada collects donations of comics , \n",
      "to distribute to children and youth in the hospital . \n",
      "\n",
      "The New Zealand government intends to hold two referendums \n",
      "to reach a verdict on the flag , \n",
      "\n",
      "a group ’s been using \n",
      "to attack the scientology website . \n",
      "\n",
      "that the baby spent all her energy \n",
      "battling the infections caused by the constant breaking of the skin , \n",
      "\n",
      "they are forced to accommodate older out - of - date technology \n",
      "to support IE6 users . \n",
      "\n",
      "but her parents moved her to the United Kingdom when she was six \n",
      "so they could be closer to the church 's headquarters . \n",
      "\n",
      "People flock \n",
      "to see the lantern scenes and enjoy the festival atmosphere \n",
      "\n",
      "which is used by officers \n",
      "to track and prosecute environmental law - breakers . \n",
      "\n",
      "by installing an oval wall with padded edges \n",
      "to protect pilgrams from a crush , \n",
      "\n",
      "Italian secularist associations are concerned the Government will modify the law \n",
      "in order to maintain an exception for religious schools . \n",
      "\n",
      "Wikinews provides additional video , audio and photographs \n",
      "so our readers may learn more . \n",
      "\n",
      "and he made lots of choices \n",
      "to go that way \" , \n",
      "\n",
      "she was willing to skip class Thursday night \n",
      "to attend the opening exhibit \n",
      "\n",
      "that need to be taken \n",
      "to improve air quality . \" \n",
      "\n",
      "But he does n’t spend a bunch of time \n",
      "looking for the bear . \n",
      "\n",
      "I got to go out on the road . \n",
      "I want to get my chops back up . \n",
      "\n",
      "while a P.A. runs out \n",
      "to buy liver & onions . \n",
      "\n",
      "I feel like they always go overboard with their dish \n",
      "to try and wow the judges \n",
      "\n",
      "and she needed it \n",
      "to relax during work breaks . \n",
      "\n",
      "Is it worth it \n",
      "to save 5 seconds x times per day \n",
      "\n",
      "to spend y minutes \n",
      "to automate it . \n",
      "\n",
      "she gave us plenty of notice \n",
      "to get a pen . \n",
      "\n",
      "and it s listening for key words \n",
      "to target advertising . \n",
      "\n",
      "connected \n",
      "to sync contacts or reset a password ... \n",
      "\n",
      "Ostriches use their wings in mating rituals , \n",
      "to make themselves appear larger , and to signal and communicate , \n",
      "\n",
      "Dude calls up \n",
      "to set up a proposal , \n",
      "\n",
      "he arrives early \n",
      "so he can hide out , \n",
      "\n",
      "Everybody crowds around the monitor \n",
      "to watch \n",
      "\n",
      "You might have to ask someone or travel a bit more \n",
      "to be accommodated , \n",
      "\n",
      "I ’d have to take a notebook to such an event \n",
      "to remember names and pronouns . \n",
      "\n",
      "Oh and a Polaroid camera \n",
      "to remember who they belong to lol . \n",
      "\n",
      "where I reasonably have ZERO time \n",
      "to do that dance . \n",
      "\n",
      "because people use money \n",
      "to buy US debt \n",
      "\n",
      "So they started to just increase the numbers on their currency \n",
      "so you can actually pay for something . \n",
      "\n",
      "I have also posted below in response to u/penny_eater \n",
      "to correct my errors and explain them in a little bit further detail \n",
      "\n",
      "No one else is stepping up \n",
      "to help solve what should be a civilization - impacting problem . \n",
      "\n",
      "b. Using these \n",
      "to release Boo into \" The Wild \" \n",
      "\n",
      "that stalk the land \n",
      "looking for lesser life to consume ? \n",
      "\n",
      "who could then work with them \n",
      "to provide any number of energy solutions that do n't involve screams . \n",
      "\n",
      "if that means they go somewhere else \n",
      "to hump \n",
      "\n",
      "where the “ smart guy ” is undergoing a polygraph \n",
      "so he can get the casino job ? \n",
      "\n",
      "Thank you for taking the time \n",
      "to read this , \n",
      "\n",
      "I ca n’t really imagine being asked which race I am \n",
      "to fill out a college application form or something like that . \n",
      "\n",
      "when they open the mouth \n",
      "to produce a vowel , \n",
      "\n",
      "So it is more important to focus on the forward clarity \n",
      "so as to not make the sound overly nasal or squeezed . \n",
      "\n",
      "I try to show up every once in a while \n",
      "to remind people that I exist \n",
      "\n",
      "they are n't going to go out of their way \n",
      "to help you succeed . \n",
      "\n",
      "that they used cardboard to block the windows \n",
      "to create the illusion that they were far away \n",
      "\n",
      "Just then , our hostess excused herself to the kitchen \n",
      "to take care of some dessert preparations . \n",
      "\n",
      "they used \n",
      "to keep an eye on ICP , etc. \n",
      "\n",
      "We 'll definitely be taking our cues from the speech therapists \n",
      "for re-teaching her language \n",
      "\n",
      "should we raise the reading level ? \n",
      "but to entertain and stimulate \n",
      "\n",
      "honestly just being there \n",
      "to encourage and support her \n",
      "\n",
      "she was on my side and there \n",
      "to help me get what I wanted . \n",
      "\n",
      "Are we brave enough and wise enough \n",
      "to grasp this opportunity and accept the challenge of the future ? \n",
      "\n",
      "and pledge ourselves to cooperate with them \n",
      "in furthering peace , freedom and democracy . \n",
      "\n",
      "Americans from every corner of the country took to the streets \n",
      "to peacefully protest violence against Black Americans , \n",
      "\n",
      "when they left \n",
      "to go to work , \n",
      "\n",
      "because they were risking their lives \n",
      "to save lives and to protect all of us . \n",
      "\n",
      "but to take further action \n",
      "to end violence and advance justice in America . \n",
      "\n",
      "we must wash our hands for 20 seconds . \n",
      "in order to be safe from COVID , \n",
      "\n",
      "we must do everything possible \n",
      "to push for reason and restraint . \n",
      "\n",
      "fought by their grandparents \n",
      "to create a republic of laws , not kings . \n",
      "\n",
      "that a juror must always be impartial \n",
      "for a trial to be fair . \n",
      "\n",
      "and appealed to Senators \n",
      "to hold this President accountable , as our Founders intended . \n",
      "\n",
      "We must act today \n",
      "in order to preserve tomorrow . \n",
      "\n",
      "to do whatever needs to be done \n",
      "to preserve this last and greatest bastion of freedom . \n",
      "\n",
      "but to try to work together every day \n",
      "to reward the faith the American people have placed in us . \n",
      "\n",
      "Public health staff will remain in touch with individuals \n",
      "to monitor the situation and give ongoing advice . \n",
      "\n",
      "have also been contacted \n",
      "to provide reassurance and answer any questions they might have ; \n",
      "\n",
      "Our border response is further ramping up \n",
      "to have health staff available for all international flights into New Zealand . \n",
      "\n",
      "they are doing \n",
      "to protect the public . \n",
      "\n",
      "We continue to work \n",
      "to keep COVID - 19 out , \n",
      "\n",
      "working on your behalf \n",
      "to achieve those hopes in the next 2 1/2 years . \n",
      "\n",
      "who joined \n",
      "in supporting my cause because they believed it was right -- \n",
      "\n",
      "and I asked the people of this city to give us their support \n",
      "to help this country move again . \n",
      "\n",
      "We are reviewing the National ICT Policy Guidelines \n",
      "to ensure alignment with proposed Sustainable Development Goals . \n",
      "\n",
      "present today \n",
      "to receive their certificates . \n",
      "\n",
      "a force led by Stephen Austin hastened to Nacogdoches \n",
      "to support the Mexican army . \n",
      "\n",
      "the delegates reconvened in early April 1833 \n",
      "to write a constitution for an independent Texas . \n",
      "\n",
      "he dispatched troops to the town of Anahuac \n",
      "to collect customs duties . \n",
      "\n",
      "Humans blink \n",
      "to keep eyes hydrated and clear of debris . \n",
      "\n",
      "American cultural anthropologist Clifford Geertz ( 1973 ) used the example of winking \n",
      "to illustrate two important aspects of culture . \n",
      "\n",
      "Use postulates of Dalton ’s atomic theory \n",
      "to explain the laws of definite and multiple proportions \n",
      "\n",
      "but apparently never considered performing experiments \n",
      "to test their ideas . \n",
      "\n",
      "and processed \n",
      "to become thoughts . \n",
      "\n",
      "used by your brain \n",
      "to organize information \n",
      "\n",
      "Let ’s look at some other examples of entrepreneurial endeavors in specific industries \n",
      "to help you plan your own venture in your own industry . \n",
      "\n",
      "The old method was to use chemical sprays \n",
      "to kill the insects , \n",
      "\n",
      "Sometimes , we must disregard our past successes and research \n",
      "to be open to new possibilities for success and failure . \n",
      "\n",
      "and is considering selling stocks \n",
      "to support significant expansion . \n",
      "\n",
      "Street ’s argument illustrates how evolutionary theory can be used \n",
      "to make a case about which metaethical theories we should adopt . \n",
      "\n",
      "Instead , they elect representatives \n",
      "to make decisions and pass laws on behalf of all the people . \n",
      "\n",
      "Residents of Boxborough , Massachusetts , gather in a local hotel \n",
      "to discuss issues affecting their town . \n",
      "\n",
      "How can I apply the Uses and Gratification Theory \n",
      "to make decisions about my learning ? \n",
      "\n",
      "as well as how to use motivation \n",
      "to purposefully take an active role in any learning activity . \n",
      "\n",
      "you will need \n",
      "to do well on the test . \n",
      "\n",
      "and second , you can check your summaries against the text \n",
      "to make certain what you know is correct and adequate . \n",
      "\n",
      "you need \n",
      "to do well on the exam . \n",
      "\n",
      "and that priest - astronomers used \n",
      "to direct the planting and harvesting of crops . \n",
      "\n",
      "The Olmec built aqueducts \n",
      "to transport water into their cities and irrigate their fields . \n",
      "\n",
      "People with the best intentions sometimes travel to a society \n",
      "to “ help ” its people , \n",
      "\n",
      "Newton ’s view of gravity is just fine \n",
      "for building bridges , skyscrapers , or amusement park rides . \n",
      "\n",
      "and still have enough information \n",
      "so tourists will not get lost . \n",
      "\n",
      "From B to C he stopped \n",
      "to receive his ticket \n",
      "\n",
      "that has been trained \n",
      "to walk in a straight line . \n",
      "\n",
      "We begin with just the rubber sheet and the ant , \n",
      "simulating empty space with no mass in it . \n",
      "\n",
      "you must order the data from smallest to largest . \n",
      "To calculate quartiles and percentiles , \n",
      "\n",
      "Percentiles are useful \n",
      "for comparing values . \n",
      "\n",
      "add the two values together and divide by two . \n",
      "To find the median , \n",
      "\n",
      "consider the same data set : \n",
      "To get the idea , \n",
      "\n",
      "The five number summary is used \n",
      "to create a box plot . \n",
      "\n",
      "The French then moved \n",
      "to capture Mexico City , \n",
      "\n",
      "Abolitionists and their Republican supporters in Congress worked \n",
      "to correct this discriminatory practice , \n",
      "\n",
      "But yeah , we 're heading to Saxby 's now \n",
      "to do work . \n",
      "\n",
      "I had booked an appointment or an e-visit with my um insurance \n",
      "to get me tested the next day . \n",
      "\n",
      "I try to smell that every morning \n",
      "to see if my change in smell changes , \n",
      "\n",
      "I have to get like this close \n",
      "to be able to smell the candle , \n",
      "\n",
      "I 'm gon na meet with her quickly \n",
      "and see if she has any feedback \n",
      "\n",
      "but I 'm too lazy \n",
      "to take it out . \n",
      "\n",
      "But I actually like when it 's gloomy \n",
      "for studying \n",
      "\n",
      "so we have three hours 45 minutes \n",
      "to take it . \n",
      "\n",
      "and we 've been getting up at five o'clock every morning \n",
      "so that we could get into the park and get hiking early . \n",
      "\n",
      "then they have the rubber soles on the front \n",
      "for if you 're walking through , like , wet grass areas \n",
      "\n",
      "we 're gon na just do a tiny bit of hiking this morning \n",
      "just to kind of get out and move , \n",
      "\n",
      "See what else we got here . \n",
      "Ah ! Alright . Let 's get into this . \n",
      "\n",
      "it 's basically just enough \n",
      "to fit a bed and a couch , \n",
      "\n",
      "that I need \n",
      "for fixing the house \n",
      "\n",
      "so we did stop \n",
      "to look at many slugs that were on the ground \n",
      "\n",
      "that it helps if you 've been tracking your period \n",
      "so you know your cycle . \n",
      "\n",
      "I 'm not grown up enough \n",
      "to do this . \n",
      "\n",
      "I get switched around -- moved around a few times \n",
      "to cover some residents who are on vacation , \n",
      "\n",
      "and take that one out as well \n",
      "to make sure there 's no metastasis . \n",
      "\n",
      "so stay tuned \n",
      "to find out which one . \n",
      "\n",
      "and go to DSW \n",
      "and see if I can exchange those Vans . \n",
      "\n",
      "I was literally in there for like 25 minutes \n",
      "all to just do a simple exchange for the exact same shoe . \n",
      "\n",
      "because they are made \n",
      "to open now , drink now wines . \n",
      "\n",
      "how – how long – when 's the best time \n",
      "to drink them ? \n",
      "\n",
      "we also got a cheese platter . Got some cheese ! \n",
      "To make it a real wine tasting , \n",
      "\n",
      "and the Areopagus appointed Draco \n",
      "to draft a strict new law code \n",
      "\n",
      "then call the local harbour master as you near the Chathams \n",
      "and he will sort you out . \n",
      "\n",
      "also utilise this wonderful national treasure \n",
      "for hosting visiting school groups . \n",
      "\n",
      "as many people from Luzon and the Visayas came \n",
      "to work either as fishermen or miners . \n",
      "\n",
      "one might suggest leaving a good 10 to 15 minutes \n",
      "to escape local traffic before hitting any major roadways . \n",
      "\n",
      "The mosque was designed \n",
      "to be a private mosque for the royal family \n",
      "\n",
      ", most people are busiest in the mornings . \n",
      "To beat the heat \n",
      "\n",
      "The center of transportation by BART and by bus is there , \n",
      "if one wants to branch out . \n",
      "\n",
      "Oakland rather than San Francisco is your best bet . \n",
      "and that to get the real essence of \" Chinatown , \" \n",
      "\n",
      "coming into the area \n",
      "to see concerts . \n",
      "\n",
      "taking the time \n",
      "to explore Denmark outside the capital . \n",
      "\n",
      "you will generally have to go back to the mainland \n",
      "to pass between the islands , \n",
      "\n",
      "It is also possible to rent Smack Dinghies with sails \n",
      "to ease the strain , \n",
      "\n",
      "Objects or donations should be placed in front of a monk \n",
      "so he can pick it up , \n",
      "\n",
      "Several freeways and bypasses can be used \n",
      "to easily get around the Tulsa Metro area : \n",
      "\n",
      "and humpback whales come to these protected waters \n",
      "to give birth . \n",
      "\n",
      "the bark of which is used \n",
      "to make tapa cloth . \n",
      "\n",
      "four days is enough \n",
      "to see the major sights \n",
      "\n",
      "or not do anything \n",
      "to stop it \n",
      "\n",
      "Ask around \n",
      "to see what they 've been saying about you . \n",
      "\n",
      "you can learn the basics \n",
      "to prepare yourself for further study . \n",
      "\n",
      "Stretching is important \n",
      "to loosen muscles , strengthen muscles and to elongate your posture . \n",
      "\n",
      "You should also stretch \n",
      "to \" wind down \" at the end of ballet dancing . \n",
      "\n",
      "It is n't made \n",
      "to tighten overly large shoes . \n",
      "\n",
      "check with your instructor \n",
      "to find out if there is a dress code at the school . \n",
      "\n",
      "Basil needs warm air and sun \n",
      "to do well , \n",
      "\n",
      "consult an almanac or talk with other gardeners in your area . \n",
      "To figure out when the last frost will be , \n",
      "\n",
      "Press the mixture slightly \n",
      "to eliminate air pockets . \n",
      "\n",
      "Cover containers with clear plastic kitchen wrap , \n",
      "so they stay moist . \n",
      "\n",
      "dig holes spaced at least 6 inches apart . \n",
      "To plant the basil in the garden , \n",
      "\n",
      "Pat down soil around the plants \n",
      "to eliminate air pockets . \n",
      "\n",
      "make sure it 's large enough \n",
      "to accommodate the number of plants you 're growing ; \n",
      "\n",
      "Weed out the smaller plants \n",
      "to give the larger ones more space . \n",
      "\n",
      "so pinch from the top . \n",
      "You want them to get bushy , \n",
      "\n",
      "Read the following steps \n",
      "to find out how to grow Beavertail cactus . \n",
      "\n",
      "unless you want to use netting \n",
      "to keep the birds off . \n",
      "\n",
      "Beavertail cacti seeds need shade \n",
      "to grow , \n",
      "\n",
      "there are holes in the center of the base of each pot \n",
      "to allow for good drainage . \n",
      "\n",
      "Put some rocks around the pad \n",
      "to anchor it in place . \n",
      "\n",
      "The moisture in the pad will provide enough nutrients \n",
      "for roots to grow . \n",
      "\n",
      "which may be an attempt to \" play dead \" \n",
      "to fool predators . \n",
      "\n",
      "Continue to hold its feet \n",
      "so the grand experiment can continue . \n",
      "\n",
      "check \n",
      "to make sure they are n't browning earlier \n",
      "\n",
      "Place cupcakes onto a wire rack \n",
      "to cool . \n",
      "\n",
      "Mix \n",
      "to combine . \n",
      "\n",
      "Stir \n",
      "to combine thoroughly . \n",
      "\n",
      "when cool enough \n",
      "to handle . \n",
      "\n",
      "One small container of pureed fruit is sufficient \n",
      "for flavoring \n",
      "\n",
      "Using the Elevator \n",
      "to Annoy People \n",
      "\n",
      "Take up as much space as you can \n",
      "to dance \n",
      "\n",
      "See Step 1 below \n",
      "to start flirting smartly and respectfully . \n",
      "\n",
      "A minute or two is great \n",
      "for breaking the ice , \n",
      "\n",
      "Use funnels , measuring tubes , and basters \n",
      "to keep everything well - measured and well away from you . \n",
      "\n",
      "Use a funnel \n",
      "to make everything safe and easy . \n",
      "\n",
      "you 're depending on \n",
      "to make the substances glow . \n",
      "\n",
      "You can write your jokes down on index cards \n",
      "to keep them handy \n",
      "\n",
      "Your setup needs to be both realistic and exaggerated \n",
      "in order to be funny — \n",
      "\n",
      "Try different things out \n",
      "and see what feels right , what sounds best . \n",
      "\n",
      "It should make some sense , \n",
      "so that it will be easier to learn and speak with \n",
      "\n",
      "Practice your language frequently \n",
      "so that you do n't forget it ! \n",
      "\n",
      "Email or text your friends the language \n",
      "so that they wo n't be so confused . \n",
      "\n",
      "Place a cotton ball under or near your garbage can \n",
      "to deter mice from getting into it . \n",
      "\n",
      "try doing a online search \n",
      "to see where it is available locally . \n",
      "\n",
      "are more likely to hunt \n",
      "to find their own food \n",
      "\n",
      "Kittens typically have to be taught by the mother \n",
      "to easily become a mouser . \n",
      "\n",
      "you can use used kitty litter \n",
      "to deter mice . \n",
      "\n",
      "a cat may not be enough \n",
      "to totally get rid of it . \n",
      "\n",
      "Build a shelter box in your garden \n",
      "to attract some onto your property . \n",
      "\n",
      "Use twist ties \n",
      "to keep the straps fastened . \n",
      "\n",
      "Take the 20 seconds \n",
      "to use a couple twist ties on the fasteners \n",
      "\n",
      "Take the 20 seconds to use a couple twist ties on the fasteners \n",
      "to make sure they do n’t come loose . \n",
      "\n",
      "Tape the bottom of the box securely \n",
      "so that it does n't give way . \n",
      "\n",
      "Boxes should be marked \" Fragile \" and \" This side up \" \n",
      "to prevent mishandling . \n",
      "\n",
      "You 'll want to find the right screw \n",
      "reassemble something \n",
      "\n",
      "Digital Photos - use your camera \n",
      "to document the contents of boxes . \n",
      "\n",
      "Re-evaluate your list halfway through your day \n",
      "to rank your items based on highest priority . \n",
      "\n",
      "Do n’t worry about completing the entire thing , but take steps now \n",
      "so that doing so in the future is a breeze . \n",
      "\n",
      "but takes two minutes \n",
      "to do - \n",
      "\n",
      "Simply force yourself to use the next 120 seconds \n",
      "to be productive \n",
      "\n",
      "Do whatever it is you ’ve been daydreaming about \n",
      "so that the temptation is removed once you get back to work . \n",
      "\n",
      "However you do n't have to have magical powers \n",
      "to play . \n",
      "\n",
      "Ideally the quaffle and bludgers should be slightly deflated \n",
      "so that they are easier to throw and catch . \n",
      "\n",
      "You do n't need flying broomsticks or magic balls \n",
      "to play this version of the game - \n",
      "\n",
      "They could also use regular - length hockey sticks \n",
      "to hit bludgers ( perhaps wiffle balls ) on the ground . \n",
      "\n",
      "the snitch is able to do whatever he or she wants \n",
      "to avoid capture . \n",
      "\n",
      "You can buy authentic looking brooms \n",
      "to make the game more realistic . \n",
      "\n",
      "have a look at the IQA website \n",
      "to find teams near you . \n",
      "\n",
      "Freshly cooked quinoa should be served immediately \n",
      "to retain nutritional value and a good flavor . \n",
      "\n",
      "Keep swirling the mixture \n",
      "until the Skittles are almost completely dissolved . \n",
      "\n",
      "You should experiment \n",
      "to find your preferred mix \n",
      "\n",
      "Pour out some vodka from a bottle \n",
      "to make room for Skittles . \n",
      "\n",
      "Shake occasionally \n",
      "so the flavour mixes well . \n",
      "\n",
      "ensuring that everything is cleaned between uses \n",
      "to prevent color contamination . \n",
      "\n",
      "Start with small amount of vodka and Skittles at first \n",
      "to experiment which combination of taste suits you best . \n",
      "\n",
      "( Or get four bottles and an empty , \n",
      "to allow room for the skittles . ) \n",
      "\n",
      "If using a t - shirt \n",
      "to strain , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# helper\n",
    "for r in df[df.converted_relation == 'Enablement'].iterrows():\n",
    "    if len(r[1].nucleus.split(' ')) < 15 and len(r[1].satellite.split(' ')) < 15:\n",
    "        print(r[1].nucleus)\n",
    "        print(r[1].satellite)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_doc(dataset, test_range, doc_name_label='doc_name'):\n",
    "    \"\"\"Split dataset into train and test set, ensuring each doc stay reside in only one.\n",
    "\n",
    "    Args:\n",
    "        dataset (DataFrame):\n",
    "        test_range (tuple): (start, end) e.g. (0, 0.2) means the first 20%\n",
    "        doc_name_label (str): label of documents' name in the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    groups = dataset.groupby(doc_name_label)\n",
    "    documents = [group for _, group in groups]\n",
    "\n",
    "    test_start_idx = int(len(documents) * test_range[0])\n",
    "    test_end_idx = int(len(documents) * test_range[1])\n",
    "\n",
    "    test_docs = documents[test_start_idx:test_end_idx]\n",
    "    train_docs = documents[:test_start_idx] + documents[test_end_idx:]\n",
    "    \n",
    "    train_df = pd.concat(train_docs).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_docs).reset_index(drop=True)\n",
    "    assert len(set(train_df[doc_name_label])) + len(set(test_df[doc_name_label])) == len(set(dataset[doc_name_label]))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = ['Attribution', 'Background', 'Cause', 'Condition', 'Contrast',\n",
    "       'Elaboration', 'Enablement', 'Evaluation', 'Explanation', 'Joint',\n",
    "       'Manner-Means', 'Same-Unit', 'Summary', 'Temporal',\n",
    "       'Textual-Organization', 'Topic-Change', 'Topic-Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_dataset(dataset, tokenizer, label_col='converted_relation'):\n",
    "    \"\"\"Turn dataframe into list(dict), dict with keys \"text\" and \"label\" \"\"\"\n",
    "\n",
    "    # get text    \n",
    "    separation_token = tokenizer.sep_token\n",
    "    input_sentences = dataset.apply(lambda x: ''.join([x['nucleus'], separation_token, x['satellite']]), axis=1)\n",
    "    np.array(input_sentences)\n",
    "\n",
    "    # get labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(label_text)\n",
    "    labels = le.transform(dataset[label_col])\n",
    "\n",
    "    data = []   \n",
    "    for text, label in zip(input_sentences, labels):\n",
    "        datapoint = {'text': text, 'label': label}\n",
    "        data.append(datapoint)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test set while preserving class distribution\n",
    "# access split data like this: for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "\n",
    "# WORKING: may try incorporating this with doc split \n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "# stk = StratifiedKFold(n_splits=5)\n",
    "# data_split = stk.split(input_sentences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_encoder_layers(model, num_frozen_layers):\n",
    "  \"\"\"Freezes the first `num_frozen_layers` of DeBERTa model.\n",
    "\n",
    "  Args:\n",
    "      model: The DeBERTa model to be fine-tuned.\n",
    "      num_frozen_layers: The number of layers to freeze.\n",
    "  \"\"\"\n",
    "  for name, param in model.named_parameters():\n",
    "    if name.startswith(\"deberta.encoder.layer.\") and int(name.split(\".\")[3]) < num_frozen_layers:\n",
    "      param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_embeddings_layers(model):\n",
    "  \"\"\"Freezes all embeddings-related layers of DeBERTa model. (no option for number of layers 'cause there's only one)\n",
    "\n",
    "  Args:\n",
    "      model: The DeBERTa model to be fine-tuned.\n",
    "  \"\"\"\n",
    "  for name, param in model.named_parameters():\n",
    "    if name.startswith(\"deberta.embeddings.\"):\n",
    "      param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_encoder_layers(model, 12)\n",
    "# freeze_embeddings_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(param.requires_grad, '-', name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAS:\n",
    "\n",
    "- nucleus and satellites currently sometimes have mixed order. May try to find a way to fix that.\n",
    "- separting nucleus and satellites from context may deprive certain information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# metric\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred): \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46910/46910 [00:08<00:00, 5777.56 examples/s]\n",
      "Map: 100%|██████████| 11768/11768 [00:01<00:00, 6076.87 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46910' max='46910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46910/46910 10:57:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.130800</td>\n",
       "      <td>2.070145</td>\n",
       "      <td>0.217071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.719900</td>\n",
       "      <td>1.728874</td>\n",
       "      <td>0.355205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.504300</td>\n",
       "      <td>1.564431</td>\n",
       "      <td>0.448048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.350200</td>\n",
       "      <td>1.415629</td>\n",
       "      <td>0.503871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.229100</td>\n",
       "      <td>1.325771</td>\n",
       "      <td>0.575048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.207400</td>\n",
       "      <td>1.207571</td>\n",
       "      <td>0.611817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.058300</td>\n",
       "      <td>1.189565</td>\n",
       "      <td>0.621984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.003100</td>\n",
       "      <td>1.152775</td>\n",
       "      <td>0.641392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>1.153674</td>\n",
       "      <td>0.643217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.963500</td>\n",
       "      <td>1.158991</td>\n",
       "      <td>0.645339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.921100</td>\n",
       "      <td>1.154052</td>\n",
       "      <td>0.651186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.923800</td>\n",
       "      <td>1.139534</td>\n",
       "      <td>0.653866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>1.136523</td>\n",
       "      <td>0.661732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>1.148070</td>\n",
       "      <td>0.667501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>1.186189</td>\n",
       "      <td>0.667419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>1.198973</td>\n",
       "      <td>0.670951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>1.212067</td>\n",
       "      <td>0.669916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.759900</td>\n",
       "      <td>1.210556</td>\n",
       "      <td>0.665851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.636400</td>\n",
       "      <td>1.231707</td>\n",
       "      <td>0.670874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>1.289146</td>\n",
       "      <td>0.668819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>1.287421</td>\n",
       "      <td>0.675108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>1.316122</td>\n",
       "      <td>0.668304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>1.309340</td>\n",
       "      <td>0.673008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>1.352679</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>1.342699</td>\n",
       "      <td>0.672339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>1.382617</td>\n",
       "      <td>0.671997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>1.400831</td>\n",
       "      <td>0.675821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.536500</td>\n",
       "      <td>1.417240</td>\n",
       "      <td>0.674251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>1.429159</td>\n",
       "      <td>0.671647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>1.426662</td>\n",
       "      <td>0.675178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.517800</td>\n",
       "      <td>1.428651</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2354' max='2354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2354/2354 06:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  {'eval_loss': 1.1365231275558472, 'eval_f1': 0.6617319636762451, 'eval_runtime': 409.0816, 'eval_samples_per_second': 28.767, 'eval_steps_per_second': 5.754, 'epoch': 5.0} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45820/45820 [00:09<00:00, 4857.22 examples/s]\n",
      "Map: 100%|██████████| 12858/12858 [00:01<00:00, 6546.71 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='241' max='45820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  241/45820 02:11 < 6:56:28, 1.82 it/s, Epoch 0.03/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 47\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# trainer object to perfrom training-related tasks\u001b[39;00m\n\u001b[1;32m     39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m=\u001b[39mcur_model,\n\u001b[1;32m     41\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_set,\n\u001b[1;32m     45\u001b[0m compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m log_history\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history)\n\u001b[1;32m     51\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_set)\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:1227\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1227\u001b[0m     label_index \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train by doc split\n",
    "\n",
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "log_history = []\n",
    "test_ranges = [(0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1)]\n",
    "\n",
    "for fold, test_range in enumerate(test_ranges): \n",
    "    train_df, test_df = split_dataset_by_doc(combined_df, test_range)\n",
    "    trainset = get_dataset(train_df, tokenizer)\n",
    "    testset = get_dataset(test_df, tokenizer)\n",
    "\n",
    "    train_set = datasets.Dataset.from_list(list(trainset))\n",
    "    test_set = datasets.Dataset.from_list(list(testset))\n",
    "\n",
    "    tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "    tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "    cur_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", \n",
    "                                                                    num_labels=17)\n",
    "    cur_model.to(torch.device('cuda'))\n",
    "\n",
    "    # training args\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=f\"./output_split_by_doc_fold_{fold + 1}\", \n",
    "    learning_rate=4e-6,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1500,\n",
    "    eval_steps=1500)\n",
    "\n",
    "    # trainer object to perfrom training-related tasks\n",
    "    trainer = Trainer(\n",
    "    model=cur_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_set,\n",
    "    eval_dataset=tokenized_test_set,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    log_history.append(trainer.state.log_history)\n",
    "\n",
    "    eval_results = trainer.evaluate(eval_dataset=tokenized_test_set)\n",
    "    print(\"Score: \", eval_results, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46942/46942 [00:07<00:00, 5879.30 examples/s]\n",
      "Map: 100%|██████████| 11736/11736 [00:01<00:00, 6238.84 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46945' max='46945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46945/46945 10:47:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.121900</td>\n",
       "      <td>2.054280</td>\n",
       "      <td>0.234579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.652100</td>\n",
       "      <td>1.622723</td>\n",
       "      <td>0.407772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.416100</td>\n",
       "      <td>1.425467</td>\n",
       "      <td>0.504743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.306400</td>\n",
       "      <td>1.304549</td>\n",
       "      <td>0.558715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.178500</td>\n",
       "      <td>1.197201</td>\n",
       "      <td>0.613545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.141900</td>\n",
       "      <td>1.175288</td>\n",
       "      <td>0.619110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.015000</td>\n",
       "      <td>1.179284</td>\n",
       "      <td>0.631917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>1.107243</td>\n",
       "      <td>0.648258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>1.066303</td>\n",
       "      <td>0.661816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>1.068191</td>\n",
       "      <td>0.663885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>1.057641</td>\n",
       "      <td>0.667650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>1.015456</td>\n",
       "      <td>0.675938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.992890</td>\n",
       "      <td>0.693320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.983880</td>\n",
       "      <td>0.701583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.995958</td>\n",
       "      <td>0.703972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.770900</td>\n",
       "      <td>0.974156</td>\n",
       "      <td>0.709687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.756600</td>\n",
       "      <td>0.943656</td>\n",
       "      <td>0.723309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.731500</td>\n",
       "      <td>0.947983</td>\n",
       "      <td>0.723976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.941879</td>\n",
       "      <td>0.725411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.937530</td>\n",
       "      <td>0.733173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.922707</td>\n",
       "      <td>0.741022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.617400</td>\n",
       "      <td>0.930654</td>\n",
       "      <td>0.736745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.943210</td>\n",
       "      <td>0.738106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.930956</td>\n",
       "      <td>0.738583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.921781</td>\n",
       "      <td>0.745375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.923751</td>\n",
       "      <td>0.746209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.915106</td>\n",
       "      <td>0.750170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.920297</td>\n",
       "      <td>0.752019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.923541</td>\n",
       "      <td>0.752325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.915123</td>\n",
       "      <td>0.754461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.921734</td>\n",
       "      <td>0.754922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2348' max='2348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2348/2348 06:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at fold: 0 : {'eval_loss': 0.9151063561439514, 'eval_f1': 0.750169557019159, 'eval_runtime': 406.1881, 'eval_samples_per_second': 28.893, 'eval_steps_per_second': 5.781, 'epoch': 5.0} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46942/46942 [00:07<00:00, 6296.76 examples/s]\n",
      "Map: 100%|██████████| 11736/11736 [00:01<00:00, 6362.36 examples/s]\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5878' max='46945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5878/46945 1:14:21 < 8:39:37, 1.32 it/s, Epoch 0.63/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.165000</td>\n",
       "      <td>2.172863</td>\n",
       "      <td>0.198610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.705500</td>\n",
       "      <td>1.642696</td>\n",
       "      <td>0.404554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.400348</td>\n",
       "      <td>0.499490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 42\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# trainer object to perfrom training-related tasks\u001b[39;00m\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m=\u001b[39mcur_model,\n\u001b[1;32m     36\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_set,\n\u001b[1;32m     40\u001b[0m compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m log_history\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history)\n\u001b[1;32m     46\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_set)\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:1227\u001b[0m, in \u001b[0;36mDebertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1227\u001b[0m     label_index \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train by random kfold\n",
    "\n",
    "# import datasets\n",
    "# from transformers import Trainer\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# log_history = []\n",
    "# for fold_number, (train_index, test_index) in enumerate(data_split):\n",
    "#     train_data = data[train_index]\n",
    "#     test_data = data[test_index]\n",
    "\n",
    "#     train_set = datasets.Dataset.from_list(list(train_data))\n",
    "#     test_set = datasets.Dataset.from_list(list(test_data))\n",
    "\n",
    "#     tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "#     tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "    \n",
    "#     cur_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", \n",
    "#                                                                     num_labels=17)\n",
    "#     cur_model.to(torch.device('cuda'))\n",
    "\n",
    "#     # training args\n",
    "#     training_args = TrainingArguments(\n",
    "#     output_dir=f\"./output_train_all_with_additional_reverse_dataset{fold_number + 1}\", \n",
    "#     learning_rate=4e-6,\n",
    "#     per_device_train_batch_size=5,\n",
    "#     per_device_eval_batch_size=5,\n",
    "#     num_train_epochs=5,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_steps=1500,\n",
    "#     eval_steps=1500)\n",
    "\n",
    "#     # trainer object to perfrom training-related tasks\n",
    "#     trainer = Trainer(\n",
    "#     model=cur_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_set,\n",
    "#     eval_dataset=tokenized_test_set,\n",
    "#     compute_metrics=compute_metrics)\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "#     log_history.append(trainer.state.log_history)\n",
    "\n",
    "#     eval_results = trainer.evaluate(eval_dataset=tokenized_test_set)\n",
    "#     print(\"Score at fold:\", fold_number, \":\", eval_results, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently Training with Reverse Dataset attached (nucleus and satellite switched place). Next design the dataset so that both version of a nucleus-satallite pair only belongs to either the traning or test set, to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.633216381072998, 'eval_f1': 0.020098247013939106}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result of training on fold 1 eval on fold 1 \n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test_set)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: To work out the evolutionary history, development and relationships among groups of organisms,[SEP]biologists compare the characteristics of living species in a process called phylogenetic analysis\n",
      "Label: Enablement\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "sample_n = \"\"\"To work out the evolutionary history, development and relationships among groups of organisms,\"\"\"\n",
    "sample_s = \"\"\"biologists compare the characteristics of living species in a process called phylogenetic analysis\"\"\"\n",
    "sample = sample_n + separation_token + sample_s\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(sample, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    output = cur_model(**tokens)\n",
    "    \n",
    "logits = output.logits\n",
    "logits = torch.Tensor.cpu(logits)\n",
    "prediction = int(np.argmax(logits))\n",
    "label = le.classes_[prediction]\n",
    "print(\"Sentence:\", sample)\n",
    "print(\"Label:\", label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Helper Blocks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
