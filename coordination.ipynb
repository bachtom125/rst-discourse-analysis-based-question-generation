{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_tree_dict(pickle_path):\n",
    "    try: \n",
    "        with open(pickle_path, 'rb') as file:\n",
    "            tree = pickle.load(file)\n",
    "            return tree\n",
    "    except:\n",
    "        print(\"Can't open file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_id(tree_dict):\n",
    "    for key, item in tree_dict.items():\n",
    "        if item['pnode_id'] == -1:\n",
    "            root_id = key\n",
    "    return root_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelib\n",
    "\n",
    "def visualize_rst_tree(tree_dict, root_id, edu_list, new_relation=False, get_edu_text=False):\n",
    "    rst_tree = treelib.Tree()\n",
    "    relation_key = 'relation' if not new_relation else 'new_relation'\n",
    "    node_list = [root_id]\n",
    "\n",
    "    while node_list:\n",
    "        id = node_list.pop()\n",
    "        node = tree_dict[id]\n",
    "        if (tree_dict.get(node['lnode_id']) is None) and (tree_dict.get(node['rnode_id']) is None):\n",
    "            node_text = \" EDU \" + str(node['edu_span'])\n",
    "            if get_edu_text:\n",
    "                node_text += \": \" + edu_list[node['edu_span'][0] - 1]\n",
    "            rst_tree.create_node(node_text, id, parent=node['pnode_id'])\n",
    "        else:\n",
    "            node_text = node['node_form']\n",
    "\n",
    "            if node['node_form'] == 'NN':\n",
    "                node_text += \"-\" + tree_dict[node['rnode_id']][relation_key]\n",
    "            elif node['node_form'] == 'NS':\n",
    "                node_text += \"-\" + tree_dict[node['rnode_id']][relation_key]\n",
    "            elif node['node_form'] == 'SN':\n",
    "                node_text += \"-\" + tree_dict[node['lnode_id']][relation_key]\n",
    "            else:\n",
    "                raise ValueError(\"Unrecognized N-S form\")\n",
    "            \n",
    "            if rst_tree.get_node(node['pnode_id']) is not None:\n",
    "                rst_tree.create_node(node_text, id, parent=node['pnode_id'])\n",
    "            else:\n",
    "                rst_tree.create_node(node_text, id)\n",
    "                print(\"\\nNo parent at node: \", node_text, '\\n')\n",
    "\n",
    "        if tree_dict.get(node['rnode_id']) is not None:\n",
    "            node_list.append(node['rnode_id'])\n",
    "        if tree_dict.get(node['lnode_id']) is not None:\n",
    "            node_list.append(node['lnode_id'])\n",
    "\n",
    "    return rst_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edus_from_file(edu_path):\n",
    "    \"\"\"Get EDUs from .edu file and return a list of EDUs\n",
    "    \"\"\"\n",
    "    edus = []\n",
    "    try: \n",
    "        with open(edu_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                edus.append(line.rstrip('\\n'))\n",
    "        return edus    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(edu_list, tree_dict, root_id): # use this when unpickling\n",
    "    \"\"\"Extract text segments from (1 or several EDUs) for relation labeller to read from (and make predictions)\n",
    "\n",
    "    Args:\n",
    "        edu_list: list containing EDUs (list)\n",
    "        tree_dict: dict containing tree (dict)\n",
    "\n",
    "    Return:\n",
    "        Dict containing text of nucleus, satellite, and original relation from StageDP (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    segments = {'pnode_id': [], 'nucleus': [], 'satellite': [], 'original_relation': []} # if multi-nuclear, satellite represent second nucleus\n",
    "    node_list = [root_id]\n",
    "    while node_list:\n",
    "        id = node_list.pop()\n",
    "        node = tree_dict[id]\n",
    "\n",
    "        if (tree_dict.get(node['lnode_id']) is None) and (tree_dict.get(node['rnode_id']) is None): # node is EDU\n",
    "            continue\n",
    "    \n",
    "        left_edu_span = tree_dict[node['lnode_id']]['edu_span'] # tuple: (from, to)\n",
    "        right_edu_span = tree_dict[node['rnode_id']]['edu_span'] # tuple: (from, to)\n",
    "        \n",
    "        # get corresponding text segments\n",
    "        left_segment = \"\"\n",
    "        for edu in range(left_edu_span[0], left_edu_span[1] + 1):\n",
    "            left_segment += edu_list[edu - 1].strip() + ' '\n",
    "\n",
    "        right_segment = \"\"\n",
    "        for edu in range(right_edu_span[0], right_edu_span[1] + 1):\n",
    "            right_segment += edu_list[edu - 1].strip() + ' '\n",
    "\n",
    "        if node['node_form'] == 'NN':\n",
    "            nucleus = left_segment\n",
    "            satellite = right_segment\n",
    "            relation = tree_dict[node['rnode_id']]['relation']\n",
    "        elif node['node_form'] == 'NS':\n",
    "            nucleus = left_segment\n",
    "            satellite = right_segment\n",
    "            relation = tree_dict[node['rnode_id']]['relation']\n",
    "        elif node['node_form'] == 'SN':\n",
    "            nucleus = right_segment\n",
    "            satellite = left_segment\n",
    "            relation = tree_dict[node['lnode_id']]['relation']\n",
    "\n",
    "        segments['nucleus'].append(nucleus)\n",
    "        segments['satellite'].append(satellite)\n",
    "        segments['original_relation'].append(relation)\n",
    "        segments['pnode_id'].append(id)  \n",
    "        \n",
    "        if tree_dict.get(node['lnode_id']) is not None:\n",
    "            node_list.append(node['lnode_id'])\n",
    "        if tree_dict.get(node['rnode_id']) is not None:\n",
    "            node_list.append(node['rnode_id'])\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def add_new_relations_to_tree_dict(tree_dict, new_relations):\n",
    "    \"\"\"Extract text segments from (1 or several EDUs) for relation labeller to read from (and make predictions)\n",
    "\n",
    "    Args:\n",
    "        tree_dict: dict containing tree (dict)\n",
    "        new_relations: df containing parent id and new relations (and other components no considered in this method)\n",
    "\n",
    "    Return:\n",
    "        New modified tree_dict according to new relations identified\n",
    "    \"\"\"\n",
    "    tree_dict_c = copy.deepcopy(tree_dict)\n",
    "    for _, r in new_relations.iterrows():\n",
    "        p_id = r['pnode_id']\n",
    "        rel = r['new_relation']\n",
    "        if tree_dict_c[p_id]['node_form'] == 'NN':\n",
    "            tree_dict_c[tree_dict_c[p_id]['rnode_id']]['new_relation'] = rel\n",
    "            tree_dict_c[tree_dict_c[p_id]['lnode_id']]['new_relation'] = rel\n",
    "        elif tree_dict_c[p_id]['node_form'] == 'NS':\n",
    "            tree_dict_c[tree_dict_c[p_id]['rnode_id']]['new_relation'] = rel\n",
    "        elif tree_dict_c[p_id]['node_form'] == 'SN':\n",
    "            tree_dict_c[tree_dict_c[p_id]['lnode_id']]['new_relation'] = rel\n",
    "        \n",
    "    return tree_dict_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_text_file(text_path, text):\n",
    "    \"\"\"Write string in text to text_path. The text is to be analyzed using RST and generated questions from. \n",
    "\n",
    "    Args:\n",
    "        text_path (str): path of file to write to\n",
    "        text (str): text to write to (informational text to extract questions from)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(text_path, 'w') as f:\n",
    "            f.write(text)\n",
    "    except:\n",
    "        print(\"Can't open text file!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to start processing text\n",
    "\n",
    "original_text = \"\"\"A transformer model is a type of deep learning model that was introduced in 2017. These models have quickly become fundamental in natural language processing (NLP), and have been applied to a wide range of tasks in machine learning and artificial intelligence.\n",
    "\n",
    "The model was first described in a 2017 paper called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto. The release of this paper is considered a watershed moment in the field, given how widespread transformers are now used in applications such as training LLMs.\n",
    "\n",
    "These models can translate text and speech in near-real-time. For example, there are apps that now allow tourists to communicate with locals on the street in their primary language. They help researchers better understand DNA and speed up drug design. They can hep detect anomalies and prevent fraud in finance and security. Vision transformers are similarly used for computer vision tasks.\n",
    "\n",
    "OpenAI’s popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more, because they allow the model to focus on the most relevant segments of input text. The “GPT” seen in the tool’s various versions (e.g. GPT-2, GPT-3) stands for “generative pre-trained transformer.” Text-based generative AI tools such as ChatGPT benefit from transformer models because they can more readily predict the next word in a sequence of text, based on a large, complex data sets.\n",
    "\n",
    "The BERT model, or Bidirectional Encoder Representations from Transformers, is based on the transformer architecture. As of 2019, BERT was used for nearly all English-language Google search results, and has been rolled out to over 70 other languages.\n",
    "\n",
    "The key innovation of the transformer model is not having to rely on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), neural network approaches which have significant drawbacks. Transformers process input sequences in parallel, making it highly efficient for training and inference — because you can’t just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM).\n",
    "\n",
    "RNNs and LSTM date back to the 1920s and 1990s, respectively. These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time. What’s more, both approaches run into limitations in retaining context when the “distance” between pieces of information in an input is long.\n",
    "\n",
    "There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text.\n",
    "\n",
    "Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information.\n",
    "\n",
    "Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can “learn” the rules of grammar, based on statistical probabilities of how words are typically used in language.\n",
    "\n",
    "Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps.\n",
    "\n",
    "Let’s imagine that you need to convert an English sentence into French. These are the steps you’d need to take to accomplish this task with a transformer model.\n",
    "\n",
    "Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings.\n",
    "\n",
    "Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information.\n",
    "\n",
    "Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism.\n",
    "\n",
    "Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training.\n",
    "\n",
    "Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data.\n",
    "\n",
    "Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data.\n",
    "\n",
    "Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence.\n",
    "\n",
    "Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD).\n",
    "\n",
    "Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.\n",
    "\n",
    "\"\"\" \n",
    "original_text = original_text.replace(u\"\\u2018\", \"'\").replace(u\"\\u2019\", \"'\").replace(u\"\\u2013\", \"-\").replace(u\"\\u2014\", \"-\").replace(u\"\\u201C\", \"-\").replace(u\"\\u201D\", \"-\") \n",
    "text_path = \"../parsers-from-github/StageDP_2/data/my_sample/sample\"\n",
    "\n",
    "write_to_text_file(text_path, original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No parent at node:  NS-Elaboration \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to process new data, adjust paths if necessary\n",
    "\n",
    "pickle_path = \"../parsers-from-github/StageDP_2/data/my_sample/sample.pickle\"\n",
    "edu_path = \"../parsers-from-github/StageDP_2/data/my_sample/sample.edus\"\n",
    "\n",
    "tree_dict = get_tree_dict(pickle_path)\n",
    "root_id = get_root_id(tree_dict)\n",
    "edus = get_edus_from_file(edu_path)\n",
    "rst_tree = visualize_rst_tree(tree_dict, root_id, edus, get_edu_text=True)\n",
    "\n",
    "# print(rst_tree.show(stdout=False, sorting=False)) # uncomment to visualize RST tree\n",
    "# print(edus)\n",
    "\n",
    "segments = extract_segments(edus, tree_dict, root_id)\n",
    "df = pd.DataFrame(segments)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembly the whole piece of text from EDUs, to ensure allignment\n",
    "\n",
    "original_text = \"\"\n",
    "for edu in edus:\n",
    "    original_text += edu + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map \"Comparison\" to \"Join\" since GUM does not contain \"Comparison\"\n",
    "for row in df[df['original_relation'] == 'Comparison'].iterrows():\n",
    "    df.at[row[0], 'original_relation'] = \"Joint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Labeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell only once\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"output_train_all_with_additional_reverse_dataset1/checkpoint-45000\"\n",
    "if 'tokenizer' not in locals(): # prevent accidental re-run of cell\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if 'model' not in locals(): # prevent accidental re-run of cell\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_text = ['Attribution', 'Background', 'Cause', 'Condition', 'Contrast',\n",
    "       'Elaboration', 'Enablement', 'Evaluation', 'Explanation', 'Joint',\n",
    "       'Manner-Means', 'Same-Unit', 'Summary', 'Temporal',\n",
    "       'Textual-Organization', 'Topic-Change', 'Topic-Comment']\n",
    "\n",
    "label_shorthand = ['Attr', 'Bckg', 'Cause', 'Cond', 'Contst',\n",
    "       'Elab', 'Enab', 'Eval', 'Expl', 'Joint',\n",
    "       'Man-Mean', 'Same-Un', 'Sum', 'Temp',\n",
    "       'Text-Org', 'Top-Chang', 'Top-Com']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(label_text)\n",
    "labels = le.transform(df.original_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding <sep> token between nucleus and satellite\n",
    "separation_token = \"[SEP]\"\n",
    "input_sentences = df.apply(lambda x: ''.join([x['nucleus'], separation_token, x['satellite']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge input sentence and labels onto one list (to form dataset object later)\n",
    "data = []\n",
    "for text in input_sentences:\n",
    "    datapoint = {'text': text}\n",
    "    data.append(datapoint)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "import datasets\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 32\n",
    "dataset = datasets.Dataset.from_list(list(data))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        tokens = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        output = model(**tokens)\n",
    "        logits = torch.Tensor.cpu(output.logits)\n",
    "        pred_labels.extend(np.argmax(logits, axis=-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = le.inverse_transform(pred_labels)\n",
    "df['new_relation'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 changed relations out of 130 (0.47)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pnode_id</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>original_relation</th>\n",
       "      <th>new_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140334317846256</td>\n",
       "      <td>A transformer model is a type of deep learning model that was introduced in 2017. These models have quickly become fundamental in natural language processing (NLP), and have been applied to a wide range of tasks in machine learning and artificial intelligence. The model was first described in a 2017 paper called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto. The release of this paper is considered a watershed moment in the field, given how widespread transformers are now used in applications such as training LLMs. These models can translate text and speech in near-real-time. For example, there are apps that now allow tourists to communicate with locals on the street in their primary language. They help researchers better understand DNA and speed up drug design. They can hep detect anomalies and prevent fraud in finance and security. Vision transformers are similarly used for computer vision tasks.</td>\n",
       "      <td>OpenAI's popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more, because they allow the model to focus on the most relevant segments of input text. The - GPT- seen in the tool's various versions (e.g. GPT-2, GPT-3) stands for - generative pre-trained transformer.- Text-based generative AI tools such as ChatGPT benefit from transformer models because they can more readily predict the next word in a sequence of text, based on a large, complex data sets. The BERT model, or Bidirectional Encoder Representations from Transformers, is based on the transformer architecture. As of 2019, BERT was used for nearly all English-language Google search results, and has been rolled out to over 70 other languages. The key innovation of the transformer model is not having to rely on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), neural network approaches which have significant drawbacks. Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively. These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time. What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long. There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140334317846144</td>\n",
       "      <td>OpenAI's popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more, because they allow the model to focus on the most relevant segments of input text. The - GPT- seen in the tool's various versions (e.g. GPT-2, GPT-3) stands for - generative pre-trained transformer.- Text-based generative AI tools such as ChatGPT benefit from transformer models because they can more readily predict the next word in a sequence of text, based on a large, complex data sets. The BERT model, or Bidirectional Encoder Representations from Transformers, is based on the transformer architecture. As of 2019, BERT was used for nearly all English-language Google search results, and has been rolled out to over 70 other languages. The key innovation of the transformer model is not having to rely on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), neural network approaches which have significant drawbacks. Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively. These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time.</td>\n",
       "      <td>What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long. There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140334317846312</td>\n",
       "      <td>What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long.</td>\n",
       "      <td>There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Joint</td>\n",
       "      <td>Contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140334317846368</td>\n",
       "      <td>There are two primary innovations that transformer models bring to the table.</td>\n",
       "      <td>Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Joint</td>\n",
       "      <td>Textual-Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140334317845920</td>\n",
       "      <td>Consider these two innovations within the context of predicting text.</td>\n",
       "      <td>Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Joint</td>\n",
       "      <td>Textual-Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>140334317791104</td>\n",
       "      <td>The model was first described in a 2017 paper called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto.</td>\n",
       "      <td>The release of this paper is considered a watershed moment in the field, given how widespread transformers are now used in applications such as training LLMs.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>140334317789872</td>\n",
       "      <td>The release of this paper is considered a watershed moment in the field,</td>\n",
       "      <td>given how widespread transformers are now used in applications such as training LLMs.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>140334317790488</td>\n",
       "      <td>given</td>\n",
       "      <td>how widespread transformers are now used in applications such as training LLMs.</td>\n",
       "      <td>Background</td>\n",
       "      <td>Attribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>140334317789536</td>\n",
       "      <td>A transformer model is a type of deep learning model that was introduced in 2017.</td>\n",
       "      <td>These models have quickly become fundamental in natural language processing (NLP), and have been applied to a wide range of tasks in machine learning and artificial intelligence.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>140334317790824</td>\n",
       "      <td>These models have quickly become fundamental in natural language processing</td>\n",
       "      <td>(NLP),</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pnode_id  \\\n",
       "0    140334317846256   \n",
       "1    140334317846144   \n",
       "2    140334317846312   \n",
       "3    140334317846368   \n",
       "4    140334317845920   \n",
       "..               ...   \n",
       "122  140334317791104   \n",
       "123  140334317789872   \n",
       "124  140334317790488   \n",
       "126  140334317789536   \n",
       "128  140334317790824   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 nucleus  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A transformer model is a type of deep learning model that was introduced in 2017. These models have quickly become fundamental in natural language processing (NLP), and have been applied to a wide range of tasks in machine learning and artificial intelligence. The model was first described in a 2017 paper called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto. The release of this paper is considered a watershed moment in the field, given how widespread transformers are now used in applications such as training LLMs. These models can translate text and speech in near-real-time. For example, there are apps that now allow tourists to communicate with locals on the street in their primary language. They help researchers better understand DNA and speed up drug design. They can hep detect anomalies and prevent fraud in finance and security. Vision transformers are similarly used for computer vision tasks.    \n",
       "1    OpenAI's popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more, because they allow the model to focus on the most relevant segments of input text. The - GPT- seen in the tool's various versions (e.g. GPT-2, GPT-3) stands for - generative pre-trained transformer.- Text-based generative AI tools such as ChatGPT benefit from transformer models because they can more readily predict the next word in a sequence of text, based on a large, complex data sets. The BERT model, or Bidirectional Encoder Representations from Transformers, is based on the transformer architecture. As of 2019, BERT was used for nearly all English-language Google search results, and has been rolled out to over 70 other languages. The key innovation of the transformer model is not having to rely on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), neural network approaches which have significant drawbacks. Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively. These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time.    \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long.    \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         There are two primary innovations that transformer models bring to the table.    \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Consider these two innovations within the context of predicting text.    \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The model was first described in a 2017 paper called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto.    \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The release of this paper is considered a watershed moment in the field,    \n",
       "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               given    \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   A transformer model is a type of deep learning model that was introduced in 2017.    \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         These models have quickly become fundamental in natural language processing    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 satellite  \\\n",
       "0    OpenAI's popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more, because they allow the model to focus on the most relevant segments of input text. The - GPT- seen in the tool's various versions (e.g. GPT-2, GPT-3) stands for - generative pre-trained transformer.- Text-based generative AI tools such as ChatGPT benefit from transformer models because they can more readily predict the next word in a sequence of text, based on a large, complex data sets. The BERT model, or Bidirectional Encoder Representations from Transformers, is based on the transformer architecture. As of 2019, BERT was used for nearly all English-language Google search results, and has been rolled out to over 70 other languages. The key innovation of the transformer model is not having to rely on recurrent neural networks (RNNs) or convolutional neural networks (CNNs), neural network approaches which have significant drawbacks. Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively. These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time. What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long. There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long. There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...   \n",
       "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The release of this paper is considered a watershed moment in the field, given how widespread transformers are now used in applications such as training LLMs.    \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 given how widespread transformers are now used in applications such as training LLMs.    \n",
       "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       how widespread transformers are now used in applications such as training LLMs.    \n",
       "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    These models have quickly become fundamental in natural language processing (NLP), and have been applied to a wide range of tasks in machine learning and artificial intelligence.    \n",
       "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (NLP),    \n",
       "\n",
       "    original_relation          new_relation  \n",
       "0         Elaboration            Background  \n",
       "1         Elaboration            Background  \n",
       "2               Joint              Contrast  \n",
       "3               Joint  Textual-Organization  \n",
       "4               Joint  Textual-Organization  \n",
       "..                ...                   ...  \n",
       "122       Elaboration            Evaluation  \n",
       "123       Elaboration           Explanation  \n",
       "124        Background           Attribution  \n",
       "126       Elaboration            Background  \n",
       "128       Elaboration               Summary  \n",
       "\n",
       "[61 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate porportion of changed labels\n",
    "\n",
    "diff = df.apply(lambda x: x['original_relation'] != x['new_relation'], axis=1)\n",
    "print(diff.sum(), \"changed relations out of\", df.shape[0], '(' + str(round(float(diff.sum()/df.shape[0]), 2)) + ')')\n",
    "df[diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No parent at node:  NS-Background \n",
      "\n",
      "NS-Background\n",
      "├── NS-Elaboration\n",
      "│   ├── NS-Background\n",
      "│   │   ├── NS-Elaboration\n",
      "│   │   │   ├──  EDU (1, 1): A transformer model is a type of deep learning model\n",
      "│   │   │   └──  EDU (2, 2): that was introduced in 2017.\n",
      "│   │   └── NN-Joint\n",
      "│   │       ├── NS-Summary\n",
      "│   │       │   ├──  EDU (3, 3): These models have quickly become fundamental in natural language processing\n",
      "│   │       │   └──  EDU (4, 4): (NLP),\n",
      "│   │       └──  EDU (5, 5): and have been applied to a wide range of tasks in machine learning and artificial intelligence.\n",
      "│   └── NS-Elaboration\n",
      "│       ├── NS-Evaluation\n",
      "│       │   ├── NS-Elaboration\n",
      "│       │   │   ├──  EDU (6, 6): The model was first described in a 2017 paper\n",
      "│       │   │   └──  EDU (7, 7): called \"Attention is All You Need\" by Ashish Vaswani, a team at Google Brain, and a group from the University of Toronto.\n",
      "│       │   └── NS-Explanation\n",
      "│       │       ├──  EDU (8, 8): The release of this paper is considered a watershed moment in the field,\n",
      "│       │       └── NN-Attribution\n",
      "│       │           ├──  EDU (9, 9): given\n",
      "│       │           └──  EDU (10, 10): how widespread transformers are now used in applications such as training LLMs.\n",
      "│       └── NS-Elaboration\n",
      "│           ├──  EDU (11, 11): These models can translate text and speech in near-real-time.\n",
      "│           └── NS-Joint\n",
      "│               ├── NS-Elaboration\n",
      "│               │   ├──  EDU (12, 12): For example, there are apps\n",
      "│               │   └──  EDU (13, 13): that now allow tourists to communicate with locals on the street in their primary language.\n",
      "│               └── NS-Joint\n",
      "│                   ├── NN-Joint\n",
      "│                   │   ├──  EDU (14, 14): They help researchers better understand DNA\n",
      "│                   │   └──  EDU (15, 15): and speed up drug design.\n",
      "│                   └── NS-Elaboration\n",
      "│                       ├── NN-Joint\n",
      "│                       │   ├──  EDU (16, 16): They can hep detect anomalies\n",
      "│                       │   └──  EDU (17, 17): and prevent fraud in finance and security.\n",
      "│                       └──  EDU (18, 18): Vision transformers are similarly used for computer vision tasks.\n",
      "└── NS-Background\n",
      "    ├── NS-Background\n",
      "    │   ├── NS-Cause\n",
      "    │   │   ├──  EDU (19, 19): OpenAI's popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering and more,\n",
      "    │   │   └── NS-Elaboration\n",
      "    │   │       ├──  EDU (20, 20): because they allow the model\n",
      "    │   │       └──  EDU (21, 21): to focus on the most relevant segments of input text.\n",
      "    │   └── NN-Textual-Organization\n",
      "    │       ├──  EDU (22, 22): The -\n",
      "    │       └── NN-Elaboration\n",
      "    │           ├── NS-Elaboration\n",
      "    │           │   ├──  EDU (23, 23): GPT- seen in the tool's various versions\n",
      "    │           │   └──  EDU (24, 24): (e.g.\n",
      "    │           └── NS-Elaboration\n",
      "    │               ├── NS-Summary\n",
      "    │               │   ├──  EDU (25, 25): GPT-2, GPT-3) stands for -\n",
      "    │               │   └── NS-Cause\n",
      "    │               │       ├──  EDU (26, 26): generative pre-trained transformer.- Text-based generative AI tools such as ChatGPT benefit from transformer models\n",
      "    │               │       └── NS-Manner-Means\n",
      "    │               │           ├──  EDU (27, 27): because they can more readily predict the next word in a sequence of text,\n",
      "    │               │           └──  EDU (28, 28): based on a large, complex data sets.\n",
      "    │               └── NS-Elaboration\n",
      "    │                   ├── NN-Same-Unit\n",
      "    │                   │   ├──  EDU (29, 29): The BERT model, or Bidirectional Encoder Representations from Transformers,\n",
      "    │                   │   └──  EDU (30, 30): is based on the transformer architecture.\n",
      "    │                   └── NS-Topic-Change\n",
      "    │                       ├── NN-Joint\n",
      "    │                       │   ├──  EDU (31, 31): As of 2019, BERT was used for nearly all English-language Google search results,\n",
      "    │                       │   └──  EDU (32, 32): and has been rolled out to over 70 other languages.\n",
      "    │                       └── NS-Explanation\n",
      "    │                           ├── NS-Elaboration\n",
      "    │                           │   ├──  EDU (33, 33): The key innovation of the transformer model is not having to rely on recurrent neural networks\n",
      "    │                           │   └── NS-Summary\n",
      "    │                           │       ├──  EDU (34, 34): (RNNs) or convolutional neural networks\n",
      "    │                           │       └── NS-Elaboration\n",
      "    │                           │           ├──  EDU (35, 35): (CNNs), neural network approaches\n",
      "    │                           │           └──  EDU (36, 36): which have significant drawbacks.\n",
      "    │                           └── NS-Contrast\n",
      "    │                               ├── NN-Topic-Change\n",
      "    │                               │   ├── NN-Explanation\n",
      "    │                               │   │   ├── NN-Cause\n",
      "    │                               │   │   │   ├──  EDU (37, 37): Transformers process input sequences in parallel,\n",
      "    │                               │   │   │   └──  EDU (38, 38): making it highly efficient for training and inference\n",
      "    │                               │   │   └── NS-Manner-Means\n",
      "    │                               │   │       ├──  EDU (39, 39): - because you can't just speed things up\n",
      "    │                               │   │       └──  EDU (40, 40): by adding more GPUs.\n",
      "    │                               │   └── NN-Background\n",
      "    │                               │       ├── NS-Summary\n",
      "    │                               │       │   ├──  EDU (41, 41): Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory\n",
      "    │                               │       │   └──  EDU (42, 42): (LSTM).\n",
      "    │                               │       └──  EDU (43, 43): RNNs and LSTM date back to the 1920s and 1990s, respectively.\n",
      "    │                               └── NN-Cause\n",
      "    │                                   ├── NS-Manner-Means\n",
      "    │                                   │   ├──  EDU (44, 44): These techniques compute each component of an input in sequence\n",
      "    │                                   │   └──  EDU (45, 45): (e.g. word by word),\n",
      "    │                                   └──  EDU (46, 46): so computation can take a long time.\n",
      "    └── NN-Contrast\n",
      "        ├── NS-Elaboration\n",
      "        │   ├──  EDU (47, 47): What's more, both approaches run into limitations\n",
      "        │   └── NS-Condition\n",
      "        │       ├──  EDU (48, 48): in retaining context\n",
      "        │       └──  EDU (49, 49): when the -distance- between pieces of information in an input is long.\n",
      "        └── NN-Textual-Organization\n",
      "            ├── NS-Elaboration\n",
      "            │   ├──  EDU (50, 50): There are two primary innovations\n",
      "            │   └──  EDU (51, 51): that transformer models bring to the table.\n",
      "            └── NN-Textual-Organization\n",
      "                ├── NS-Elaboration\n",
      "                │   ├──  EDU (52, 52): Consider these two innovations within the context\n",
      "                │   └──  EDU (53, 53): of predicting text.\n",
      "                └── NN-Joint\n",
      "                    ├── NN-Same-Unit\n",
      "                    │   ├──  EDU (54, 54): Positional encoding: Instead\n",
      "                    │   └── SN-Manner-Means\n",
      "                    │       ├── NS-Elaboration\n",
      "                    │       │   ├──  EDU (55, 55): of looking at each word in the order\n",
      "                    │       │   └──  EDU (56, 56): that it appears in a sentence,\n",
      "                    │       └──  EDU (57, 57): a unique number is assigned to each word.\n",
      "                    └── NN-Joint\n",
      "                        ├── NS-Cause\n",
      "                        │   ├── NS-Elaboration\n",
      "                        │   │   ├──  EDU (58, 58): This provides information about the position of each token\n",
      "                        │   │   └──  EDU (59, 59): (parts of the input such as words or subword pieces in NLP) in the sequence,\n",
      "                        │   └── NS-Elaboration\n",
      "                        │       ├──  EDU (60, 60): allowing the model\n",
      "                        │       └──  EDU (61, 61): to consider the sequence's sequential information.\n",
      "                        └── NN-Elaboration\n",
      "                            ├── NN-Elaboration\n",
      "                            │   ├── NS-Elaboration\n",
      "                            │   │   ├──  EDU (62, 62): Self-attention: Attention is a mechanism\n",
      "                            │   │   └── NS-Enablement\n",
      "                            │   │       ├── NS-Background\n",
      "                            │   │       │   ├──  EDU (63, 63): that calculates weights for every word in a sentence\n",
      "                            │   │       │   └──  EDU (64, 64): as they relate to every other word in the sentence,\n",
      "                            │   │       └── NS-Elaboration\n",
      "                            │   │           ├──  EDU (65, 65): so the model can predict words\n",
      "                            │   │           └──  EDU (66, 66): which are likely to be used in sequence.\n",
      "                            │   └── NS-Elaboration\n",
      "                            │       ├── NS-Background\n",
      "                            │       │   ├──  EDU (67, 67): This understanding is learned over time\n",
      "                            │       │   └── NN-Same-Unit\n",
      "                            │       │       ├──  EDU (68, 68): as a model\n",
      "                            │       │       └──  EDU (69, 69): is trained on lots of data.\n",
      "                            │       └── NS-Elaboration\n",
      "                            │           ├── NS-Same-Unit\n",
      "                            │           │   ├──  EDU (70, 70): The self-attention mechanism allows each word\n",
      "                            │           │   └── NS-Manner-Means\n",
      "                            │           │       ├──  EDU (71, 71): to attend to every other word in the sequence in parallel,\n",
      "                            │           │       └──  EDU (72, 72): weighing their importance for the current token.\n",
      "                            │           └── NS-Summary\n",
      "                            │               ├── SN-Attribution\n",
      "                            │               │   ├──  EDU (73, 73): In this way, it can be said\n",
      "                            │               │   └── NS-Manner-Means\n",
      "                            │               │       ├──  EDU (74, 74): that machine learning models can -learn- the rules of grammar,\n",
      "                            │               │       └── NS-Elaboration\n",
      "                            │               │           ├──  EDU (75, 75): based on statistical probabilities\n",
      "                            │               │           └──  EDU (76, 76): of how words are typically used in language.\n",
      "                            │               └── NS-Elaboration\n",
      "                            │                   ├── SN-Manner-Means\n",
      "                            │                   │   ├──  EDU (77, 77): Transformer models work\n",
      "                            │                   │   └── NS-Elaboration\n",
      "                            │                   │       ├──  EDU (78, 78): by processing input data,\n",
      "                            │                   │       └── NS-Elaboration\n",
      "                            │                   │           ├──  EDU (79, 79): which can be sequences of tokens or other structured data, through a series of layers\n",
      "                            │                   │           └── NN-Same-Unit\n",
      "                            │                   │               ├──  EDU (80, 80): that contain self-attention mechanisms\n",
      "                            │                   │               └──  EDU (81, 81): and feedforward neural networks.\n",
      "                            │                   └── NS-Textual-Organization\n",
      "                            │                       ├── NN-Same-Unit\n",
      "                            │                       │   ├── NS-Elaboration\n",
      "                            │                       │   │   ├──  EDU (82, 82): The core idea\n",
      "                            │                       │   │   └──  EDU (83, 83): behind how transformer models work\n",
      "                            │                       │   └──  EDU (84, 84): can be broken down into several key steps.\n",
      "                            │                       └── NS-Background\n",
      "                            │                           ├── SN-Attribution\n",
      "                            │                           │   ├──  EDU (85, 85): Let's imagine\n",
      "                            │                           │   └── NS-Enablement\n",
      "                            │                           │       ├──  EDU (86, 86): that you need\n",
      "                            │                           │       └──  EDU (87, 87): to convert an English sentence into French.\n",
      "                            │                           └── NS-Elaboration\n",
      "                            │                               ├──  EDU (88, 88): These are the steps\n",
      "                            │                               └── NS-Enablement\n",
      "                            │                                   ├──  EDU (89, 89): you'd need to take\n",
      "                            │                                   └──  EDU (90, 90): to accomplish this task with a transformer model.\n",
      "                            └── NS-Elaboration\n",
      "                                ├── NS-Summary\n",
      "                                │   ├──  EDU (91, 91): Input embeddings: The input sentence is first transformed into numerical representations\n",
      "                                │   └──  EDU (92, 92): called embeddings.\n",
      "                                └── NS-Joint\n",
      "                                    ├── NS-Elaboration\n",
      "                                    │   ├──  EDU (93, 93): These capture the semantic meaning of the tokens in the input sequence.\n",
      "                                    │   └── NN-Joint\n",
      "                                    │       ├──  EDU (94, 94): For sequences of words, these embeddings can be learned during training\n",
      "                                    │       └──  EDU (95, 95): or obtained from pre-trained word embeddings.\n",
      "                                    └── NN-Textual-Organization\n",
      "                                        ├──  EDU (96, 96): Positional encoding:\n",
      "                                        └── NS-Elaboration\n",
      "                                            ├── NS-Elaboration\n",
      "                                            │   ├──  EDU (97, 97): Positional encoding is typically introduced as a set of additional values or vectors\n",
      "                                            │   └── NS-Background\n",
      "                                            │       ├──  EDU (98, 98): that are added to the token embeddings\n",
      "                                            │       └──  EDU (99, 99): before feeding them into the transformer model.\n",
      "                                            └── NN-Elaboration\n",
      "                                                ├── NS-Elaboration\n",
      "                                                │   ├──  EDU (100, 100): These positional encodings have specific patterns\n",
      "                                                │   └──  EDU (101, 101): that encode the position information.\n",
      "                                                └── NN-Joint\n",
      "                                                    ├── NS-Enablement\n",
      "                                                    │   ├──  EDU (102, 102): Multi-head attention: Self-attention operates in multiple \"attention heads\"\n",
      "                                                    │   └──  EDU (103, 103): to capture different types of relationships between tokens.\n",
      "                                                    └── NN-Joint\n",
      "                                                        ├── NN-Same-Unit\n",
      "                                                        │   ├──  EDU (104, 104): Softmax functions, a type of activation function,\n",
      "                                                        │   └── NS-Enablement\n",
      "                                                        │       ├──  EDU (105, 105): are used\n",
      "                                                        │       └──  EDU (106, 106): to calculate attention weights in the self-attention mechanism.\n",
      "                                                        └── NN-Joint\n",
      "                                                            ├── NS-Enablement\n",
      "                                                            │   ├──  EDU (107, 107): Layer normalization and residual connections: The model uses layer normalization and residual connections\n",
      "                                                            │   └──  EDU (108, 108): to stabilize and speed up training.\n",
      "                                                            └── NS-Textual-Organization\n",
      "                                                                ├──  EDU (109, 109): Feedforward neural networks:\n",
      "                                                                └── NS-Elaboration\n",
      "                                                                    ├──  EDU (110, 110): The output of the self-attention layer is passed through feedforward layers.\n",
      "                                                                    └── NS-Elaboration\n",
      "                                                                        ├── NS-Cause\n",
      "                                                                        │   ├──  EDU (111, 111): These networks apply non-linear transformations to the token representations,\n",
      "                                                                        │   └──  EDU (112, 112): allowing the model to capture complex patterns and relationships in the data.\n",
      "                                                                        └── NS-Elaboration\n",
      "                                                                            ├── NS-Elaboration\n",
      "                                                                            │   ├──  EDU (113, 113): Stacked layers: Transformers typically consist of multiple layers\n",
      "                                                                            │   └──  EDU (114, 114): stacked on top of each other.\n",
      "                                                                            └── NS-Elaboration\n",
      "                                                                                ├── NS-Manner-Means\n",
      "                                                                                │   ├──  EDU (115, 115): Each layer processes the output of the previous layer,\n",
      "                                                                                │   └──  EDU (116, 116): gradually refining the representations.\n",
      "                                                                                └── NS-Topic-Change\n",
      "                                                                                    ├── NS-Elaboration\n",
      "                                                                                    │   ├──  EDU (117, 117): Stacking multiple layers enables the model\n",
      "                                                                                    │   └──  EDU (118, 118): to capture hierarchical and abstract features in the data.\n",
      "                                                                                    └── NS-Textual-Organization\n",
      "                                                                                        ├──  EDU (119, 119): Output layer:\n",
      "                                                                                        └── NS-Topic-Change\n",
      "                                                                                            ├── NS-Enablement\n",
      "                                                                                            │   ├──  EDU (120, 120): In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder\n",
      "                                                                                            │   └──  EDU (121, 121): to generate the output sequence.\n",
      "                                                                                            └── NN-Temporal\n",
      "                                                                                                ├── NS-Manner-Means\n",
      "                                                                                                │   ├──  EDU (122, 122): Training: Transformer models are trained\n",
      "                                                                                                │   └── NS-Manner-Means\n",
      "                                                                                                │       ├──  EDU (123, 123): using supervised learning,\n",
      "                                                                                                │       └── NS-Elaboration\n",
      "                                                                                                │           ├──  EDU (124, 124): where they learn to minimize a loss function\n",
      "                                                                                                │           └──  EDU (125, 125): that quantifies the difference between the model's predictions and the ground truth for the given task.\n",
      "                                                                                                └── NN-Elaboration\n",
      "                                                                                                    ├── NS-Summary\n",
      "                                                                                                    │   ├──  EDU (126, 126): Training typically involves optimization techniques like Adam or stochastic gradient descent\n",
      "                                                                                                    │   └──  EDU (127, 127): (SGD).\n",
      "                                                                                                    └── NS-Textual-Organization\n",
      "                                                                                                        ├──  EDU (128, 128): Inference:\n",
      "                                                                                                        └── NS-Temporal\n",
      "                                                                                                            ├──  EDU (129, 129): After training, the model can be used for inference on new data.\n",
      "                                                                                                            └── NN-Temporal\n",
      "                                                                                                                ├──  EDU (130, 130): During inference, the input sequence is passed through the pre-trained model,\n",
      "                                                                                                                └──  EDU (131, 131): and the model generates predictions or representations for the given task.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fix old tree with new relations\n",
    "\n",
    "new_tree_dict = add_new_relations_to_tree_dict(tree_dict, df)\n",
    "new_rst_tree = visualize_rst_tree(new_tree_dict, get_root_id(new_tree_dict), edus, new_relation=True, get_edu_text=True)\n",
    "\n",
    "print(new_rst_tree.show(stdout=False, sorting=False)) # for visualizing rst_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 16 artists>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_cnt = pd.Series(pred_labels).value_counts()\n",
    "indices = pd.Series(label_shorthand).reindex(label_cnt.index, fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.yticks(np.arange(1, max(label_cnt) + 1))\n",
    "plt.bar(indices, label_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pnode_id</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>original_relation</th>\n",
       "      <th>new_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>140334317843176</td>\n",
       "      <td>Each layer processes the output of the previous layer,</td>\n",
       "      <td>gradually refining the representations.</td>\n",
       "      <td>Cause</td>\n",
       "      <td>Manner-Means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>140334317792000</td>\n",
       "      <td>This understanding is learned over time</td>\n",
       "      <td>as a model is trained on lots of data.</td>\n",
       "      <td>Cause</td>\n",
       "      <td>Background</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pnode_id                                                  nucleus  \\\n",
       "35  140334317843176  Each layer processes the output of the previous layer,    \n",
       "70  140334317792000                 This understanding is learned over time    \n",
       "\n",
       "                                   satellite original_relation  new_relation  \n",
       "35  gradually refining the representations.              Cause  Manner-Means  \n",
       "70   as a model is trained on lots of data.              Cause    Background  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find occurences of a specific relation\n",
    "\n",
    "df[df[\"original_relation\"] == \"Cause\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pnode_id</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>original_relation</th>\n",
       "      <th>new_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140334317846312</td>\n",
       "      <td>What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long.</td>\n",
       "      <td>There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.</td>\n",
       "      <td>Joint</td>\n",
       "      <td>Contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>140334317791048</td>\n",
       "      <td>Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively.</td>\n",
       "      <td>These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time.</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>Contrast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pnode_id  \\\n",
       "2   140334317846312   \n",
       "93  140334317791048   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                        nucleus  \\\n",
       "2                                                                                                                                                                                                                                What's more, both approaches run into limitations in retaining context when the -distance- between pieces of information in an input is long.    \n",
       "93  Transformers process input sequences in parallel, making it highly efficient for training and inference - because you can't just speed things up by adding more GPUs. Transformer models need less training time than previous recurrent neural network architectures such as long short-term memory (LSTM). RNNs and LSTM date back to the 1920s and 1990s, respectively.    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               satellite  \\\n",
       "2   There are two primary innovations that transformer models bring to the table. Consider these two innovations within the context of predicting text. Positional encoding: Instead of looking at each word in the order that it appears in a sentence, a unique number is assigned to each word. This provides information about the position of each token (parts of the input such as words or subword pieces in NLP) in the sequence, allowing the model to consider the sequence's sequential information. Self-attention: Attention is a mechanism that calculates weights for every word in a sentence as they relate to every other word in the sentence, so the model can predict words which are likely to be used in sequence. This understanding is learned over time as a model is trained on lots of data. The self-attention mechanism allows each word to attend to every other word in the sequence in parallel, weighing their importance for the current token. In this way, it can be said that machine learning models can -learn- the rules of grammar, based on statistical probabilities of how words are typically used in language. Transformer models work by processing input data, which can be sequences of tokens or other structured data, through a series of layers that contain self-attention mechanisms and feedforward neural networks. The core idea behind how transformer models work can be broken down into several key steps. Let's imagine that you need to convert an English sentence into French. These are the steps you'd need to take to accomplish this task with a transformer model. Input embeddings: The input sentence is first transformed into numerical representations called embeddings. These capture the semantic meaning of the tokens in the input sequence. For sequences of words, these embeddings can be learned during training or obtained from pre-trained word embeddings. Positional encoding: Positional encoding is typically introduced as a set of additional values or vectors that are added to the token embeddings before feeding them into the transformer model. These positional encodings have specific patterns that encode the position information. Multi-head attention: Self-attention operates in multiple \"attention heads\" to capture different types of relationships between tokens. Softmax functions, a type of activation function, are used to calculate attention weights in the self-attention mechanism. Layer normalization and residual connections: The model uses layer normalization and residual connections to stabilize and speed up training. Feedforward neural networks: The output of the self-attention layer is passed through feedforward layers. These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data. Stacked layers: Transformers typically consist of multiple layers stacked on top of each other. Each layer processes the output of the previous layer, gradually refining the representations. Stacking multiple layers enables the model to capture hierarchical and abstract features in the data. Output layer: In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder to generate the output sequence. Training: Transformer models are trained using supervised learning, where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task. Training typically involves optimization techniques like Adam or stochastic gradient descent (SGD). Inference: After training, the model can be used for inference on new data. During inference, the input sequence is passed through the pre-trained model, and the model generates predictions or representations for the given task.    \n",
       "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            These techniques compute each component of an input in sequence (e.g. word by word), so computation can take a long time.    \n",
       "\n",
       "   original_relation new_relation  \n",
       "2              Joint     Contrast  \n",
       "93       Elaboration     Contrast  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find occurences of a specific relation\n",
    "\n",
    "df[df[\"new_relation\"] == \"Contrast\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b3e92e4e5fd54f0899f9a32191af27bc-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">model</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">trained</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">using</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">supervised</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">learning.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b3e92e4e5fd54f0899f9a32191af27bc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "if 'nlp' not in locals(): # prevent accidental re-run of cell\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "sent = \"the model is trained using supervised learning.\"\n",
    "doc = nlp(sent)\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[672], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, token\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, token\u001b[38;5;241m.\u001b[39ment_type_ , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, token\u001b[38;5;241m.\u001b[39mdep_)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.children., '-', token.text, '-', token.ent_type_ , '-', token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - I\n",
      "Tom - Tom\n",
      "midnight - midnight\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.root, '-', chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun phrase'"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Helper Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enablement'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sep = tokenizer.sep_token\n",
    "text_n = \"She used the tool in the garden, \"\n",
    "text_s = \"in order to win.\"\n",
    "text = text_n + sep + text_s\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    output = model(**tokens)\n",
    "    logits = torch.Tensor.cpu(output.logits)\n",
    "    single_pred = int(np.argmax(logits, axis=-1))\n",
    "    \n",
    "le.inverse_transform([single_pred])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIVE_PRONOUNS = ['who', 'that', 'whose', 'which']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp_small = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj(clause, accept_pron=True, accept_expl=False):\n",
    "    \"\"\"Return subject of clause, None of none found\n",
    "\n",
    "    Args:\n",
    "        clause (str): \n",
    "        accept_expl (bool, optional): If take expletive as subject. Defaults to False.\n",
    "        accept_expl (bool, optional): If take pronoun as subject. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The subject\n",
    "    \"\"\"\n",
    "    doc = nlp(clause)\n",
    "\n",
    "    for token in doc:\n",
    "        if 'nsubj' in token.dep_:\n",
    "            if token.pos_ == \"PRON\" and not accept_pron:\n",
    "                continue\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if chunk.start <= token.i and token.i < chunk.end:\n",
    "                    return chunk.text\n",
    "        if accept_expl:\n",
    "            if 'expl' in token.dep_:\n",
    "                return token.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING: make a more fool proof method of checking relative clause: inputing both source sents and text, and use pos tags to determine, refer to : is_dependent_clause()\n",
    "def check_relative_clause(text):\n",
    "    \"\"\"Return if the text (has to contain only one clause) is a relative clause, \n",
    "    relative clauses can start with \"which\", \"Ving, \"Ved\" (not including adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        text\n",
    "    Return\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if (token.dep_ == \"nsubj\"):\n",
    "            if (token.text.lower() in RELATIVE_PRONOUNS):\n",
    "                return 1 # relative clause starting with relative pronouns\n",
    "            return 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (token.pos_ == \"VERB\") and (token.tag_ in ['VBG', 'VBN']):\n",
    "                # WORKING: Change way to check tense, use tag_ instead of morph\n",
    "                if len(token.morph.get('Tense')) == 0:\n",
    "                    return 0 # not relative clause\n",
    "                if (token.text.endswith('ing')) or (token.tag_ ==  'VBN'):\n",
    "                    return 2 # shortened relative clause (ending with Ving or Ved)\n",
    "            else: \n",
    "                return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_relative_clause(\"where they learn to minimize a loss function that quantifies the difference between the model's predictions and the ground truth for the given task.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundary(sent_doc, text_doc):\n",
    "    \"\"\"Find boundary indices of text_doc in sent_doc (given that text_doc is in sent_doc)\n",
    "\n",
    "    Args:\n",
    "        sent_doc (nlp Doc): \n",
    "        text_doc (nlp Doc): \n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    while start_ind < len(sent_doc):\n",
    "        if text_doc.text not in sent_doc[start_ind:].text:\n",
    "            break\n",
    "        start_ind += 1\n",
    "    start_ind -= 1\n",
    "\n",
    "    end_ind = len(sent_doc)\n",
    "    while end_ind > 0:\n",
    "        if (text_doc.text not in sent_doc[:end_ind].text) or (end_ind <= start_ind):\n",
    "            break\n",
    "        end_ind -= 1\n",
    "    end_ind += 1\n",
    "\n",
    "    return start_ind, end_ind\n",
    "\n",
    "def is_dependent_clause(src_sent, text):\n",
    "    # WORKING: may use this way to check relative clause as well\n",
    "    sent_doc = nlp(src_sent)\n",
    "    text_doc = nlp(text)   \n",
    "\n",
    "    start_ind, end_ind = find_boundary(sent_doc, text_doc)\n",
    "    if start_ind < 0 or end_ind > len(sent_doc):\n",
    "        print(\"\\nText not found in source sentence!\\n\")\n",
    "        return None\n",
    "\n",
    "    for token in sent_doc[start_ind:end_ind]:\n",
    "        if token.dep_ in ['acl', 'ccomp', 'advcl', 'xcomp']:\n",
    "            if token.head.i in range(start_ind, end_ind):\n",
    "                return False\n",
    "            return True\n",
    "    return False     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 in range(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Training: Transformer models are trained using which is supervised learning',\n",
       " 'using which is supervised learning')"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unshorten_relative_clause(\"Training: Transformer models are trained using supervised learning\", \"using supervised learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def unshorten_relative_clause(original_sent, clause):\n",
    "    \"\"\"Unshorten relative clause (make sure it's a relative clause before calling this method) ending with Ving or Ved, convert them to which/who + V\n",
    "\n",
    "    Args:\n",
    "        clause (str): relative clause containing Ving or Ved\n",
    "        original_sent (str): original sentence containing that clause\n",
    "    Return:\n",
    "        tuple(str, str): modified clause and source sentence\n",
    "    \"\"\"\n",
    "\n",
    "    clause_doc = nlp(clause)\n",
    "    text_doc = nlp(original_sent)\n",
    "    vb = \"\"\n",
    "\n",
    "    for token in clause_doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (token.pos_ == \"VERB\") and (token.tag_ in ['VBG', 'VBN']): # finds Ving or Ved\n",
    "                vb = token\n",
    "\n",
    "    for v_token in text_doc:\n",
    "        # find verb in original sentence\n",
    "        if v_token.text == vb.text.strip():\n",
    "            c_i = vb.i\n",
    "            t_i = v_token.i\n",
    "            is_verb = True\n",
    "            while c_i < len(clause_doc) and t_i < len(text_doc):\n",
    "                if clause_doc[c_i].text.strip() != text_doc[t_i].text.strip():\n",
    "                    is_verb = False\n",
    "                    break\n",
    "                c_i += 1\n",
    "                t_i += 1\n",
    "            if is_verb:\n",
    "                break\n",
    "    \n",
    "    pointed_noun = v_token.head\n",
    "    pointed_noun_chunk = None\n",
    "    for chunk in text_doc.noun_chunks:\n",
    "        if chunk.start <= pointed_noun.i and pointed_noun.i < chunk.end:\n",
    "            pointed_noun_chunk = chunk\n",
    "    \n",
    "    if pointed_noun_chunk:\n",
    "        pointed_root_noun = pointed_noun_chunk.root\n",
    "    else:\n",
    "        pointed_root_noun = pointed_noun\n",
    "    \n",
    "    rel_pro = \"which\" \n",
    "    # not a fool-proof way to determine if noun is person\n",
    "    if pointed_root_noun.ent_type_:\n",
    "        if pointed_root_noun.ent_type_ == \"PERSON\":\n",
    "            rel_pro = \"who\"\n",
    "\n",
    "    # check plurality of noun/pronoun and conjugate accordingly\n",
    "    # only applicable for Present Tense, not for past or others\n",
    "    if pointed_root_noun.pos_.startswith(\"NOUN\"): # noun\n",
    "        plurality = pointed_root_noun.tag_ == \"NNS\"\n",
    "    elif \"PRON\" in pointed_root_noun.pos_:  # pronoun\n",
    "        plurality = (pointed_root_noun.lemma_ == \"we\") or (pointed_root_noun.lemma_ == \"you\") or (pointed_root_noun.lemma_ == \"they\") or ((pointed_root_noun.lemma_ == \"I\"))\n",
    "    else: # WORKING: for other cases where relative pronoun does not point to a noun, but a verb or a clause\n",
    "        plurality = False # temporary solution\n",
    "\n",
    "    if not plurality:\n",
    "        if vb.tag_ == 'VBG':\n",
    "            conj_vb = p.plural_noun(vb.lemma_) # get singular conjugation (plural_noun() method works with verbs too)\n",
    "        else:\n",
    "            if pointed_root_noun.text.strip() == \"I\":\n",
    "                aux = 'am'\n",
    "            else:\n",
    "                aux = 'is'\n",
    "            conj_vb = aux + ' ' + vb.text\n",
    "    else:\n",
    "        if vb.tag_ == 'VGB':\n",
    "            conj_vb = p.plural_verb(vb.lemma_) # get plural conjugation\n",
    "        else:\n",
    "            if pointed_root_noun.text.strip() == \"I\":\n",
    "                aux = 'am'\n",
    "            else:\n",
    "                aux = 'are'\n",
    "            conj_vb = aux + ' ' + vb.text\n",
    "\n",
    "    fixed_clause = clause.replace(vb.text, rel_pro + ' ' +  conj_vb, 1)# replace only the first occurence of the verb\n",
    "    fixed_sent = original_sent.replace(clause.strip(), fixed_clause.strip(), 1)\n",
    "    return  (fixed_sent, fixed_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('I saw Tom which walks at midnight', 'which walks at midnight')"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unshorten_relative_clause(\"I saw Tom walking at midnight\", \"walking at midnight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING: Need to handle all types of shortened relative clauses, for now, only Ving and Ved is covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_one_clause(text, count_relative_clause=True):\n",
    "    \"\"\"Check if input text is one clause or multiple.\n",
    "        NOTE: If not multiple clause, the method returns true, so does not account for the case of not a full clause, just check whether multiple clauses or not, cause a EDU is usually at least a clause semantically. \n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # check how many subjects\n",
    "    has_subj = False\n",
    "    for token in doc:\n",
    "        if \"nsubj\" in token.dep_:\n",
    "            if not count_relative_clause:\n",
    "                if token.text.lower() in ['who', 'whom', 'whose', 'which', 'that']:\n",
    "                    continue\n",
    "                if token.head.dep_ in ['relcl', 'acl', 'ccomp']:\n",
    "                    continue\n",
    "            if has_subj:\n",
    "                return False\n",
    "            else:\n",
    "                has_subj = True\n",
    "    return True # not return has_subj, so that even no subject will be 1 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_aux(sentence):\n",
    "    \"\"\"Check if sentence has auxiliary verb.\n",
    "    Input one sentence only.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if \"AUX\" in token.pos_:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_aux_to_beginning(sentence):\n",
    "    \"\"\"Move auxiliary verb to the beginning of sentence (to form question).\n",
    "    Input one sentence only. Make sure it has aux verb. Make sure sentence starts with subject.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    aux = \"\"\n",
    "    for token in doc:\n",
    "        if \"AUX\" in token.pos_:\n",
    "            aux = token.text\n",
    "            break\n",
    "    assert len(aux)\n",
    "\n",
    "\n",
    "    new_sent = aux + ' ' + sentence.strip().replace(aux, '', 1).replace(sentence[0], sentence[0].lower(), 1)\n",
    "\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can the person on the right  be her father, I can do it if not.\n"
     ]
    }
   ],
   "source": [
    "# move aux test run\n",
    "\n",
    "txt = \"The person on the right can be her father, I can do it if not.\"\n",
    "if has_aux(txt):\n",
    "    print(move_aux_to_beginning(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_verb(sentence):\n",
    "    \"\"\"Check if sentence has normal verb.\n",
    "    Input one sentence only.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if \"VERB\" in token.pos_:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_and_replace_aux_for_verb(sentence):\n",
    "    \"\"\"Put appropriate aux at beginnging of clause \n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    main_verb = None\n",
    "\n",
    "    for token in doc:\n",
    "        if \"VERB\" in token.pos_:\n",
    "            main_verb = token\n",
    "            break\n",
    "            \n",
    "    if not main_verb:\n",
    "        print(\"\\nCan't find main verb in\", sentence, '!\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    # check plurality\n",
    "    pointed_noun = main_verb.head\n",
    "    if pointed_noun.pos_.startswith(\"NOUN\"): # noun\n",
    "        plurality = pointed_noun.tag_ == \"NNS\"\n",
    "    elif pointed_noun.pos_.startswith(\"PRP\"):  # pronoun\n",
    "        plurality = (pointed_noun.lemma_ == \"we\") or (pointed_noun.lemma_ == \"you\") or (pointed_noun.lemma_ == \"they\")\n",
    "    else: # WORKING: for other cases where relative pronoun does not point to a noun, but a verb or a clause\n",
    "        plurality = True # temporary solution\n",
    "\n",
    "    # check tense\n",
    "    tense = \"present\" if main_verb.tag_ in ['VBZ', 'VBP'] else 'past'\n",
    "    # get aux\n",
    "    aux = {\n",
    "      \"present\": \"do\" if plurality else \"does\",\n",
    "      \"past\": \"did\",\n",
    "    }.get(tense)\n",
    "\n",
    "    # replace appropriate auxilary\n",
    "    new_sent = sentence.replace(main_verb.text, main_verb.lemma_, 1)\n",
    "    new_sent = aux + ' ' + new_sent\n",
    "\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did he hit down.\n"
     ]
    }
   ],
   "source": [
    "# move verb test run\n",
    "\n",
    "txt = \"he hit down.\"\n",
    "if has_verb(txt):\n",
    "    print(choose_and_replace_aux_for_verb(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ending_special_chars(sentence):\n",
    "    \"\"\"Remove ending non-word characters of a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to be stripped\n",
    "\n",
    "    Returns:\n",
    "        str: stripped sentence \n",
    "    \"\"\"\n",
    "    sen_len = len(sentence)\n",
    "    for i in range(sen_len - 1, -1, -1):\n",
    "        char = sentence[i]\n",
    "\n",
    "        # check if the character is a punctuation mark\n",
    "        if char.isalnum():\n",
    "            return sentence\n",
    "        else:\n",
    "            sentence = sentence[:i]\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_special_chars(sentence):\n",
    "    \"\"\"Remove leading non-word characters of a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to be stripped\n",
    "\n",
    "    Returns:\n",
    "        str: stripped sentence \n",
    "    \"\"\"\n",
    "\n",
    "    start_ind = 0\n",
    "    for i in range(0, len(sentence)):\n",
    "        char = sentence[i]\n",
    "        # check if the character is a punctuation mark\n",
    "        if char.isalnum():\n",
    "            break\n",
    "        else:\n",
    "            start_ind = i + 1\n",
    "\n",
    "    return sentence[start_ind: ].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentence\n",
    "\n",
    "    Args:\n",
    "        text (str): text to be plited\n",
    "    Return: \n",
    "        list[str]: spit text\n",
    "    \"\"\"\n",
    "    sents = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\", text)\n",
    "    sents = [sent for sent in sents if len(sent.strip())]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_source_sents(sents, text, span=2):\n",
    "    \"\"\"Find the sentences of which the text is a part. \n",
    "\n",
    "    Args:\n",
    "        sents (str): \n",
    "        text (str): \n",
    "        span (int): span of sentences to the left to return\n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    while start_ind < len(sents):\n",
    "        if text not in ''.join(sents[start_ind:]):\n",
    "            break\n",
    "        start_ind += 1\n",
    "    start_ind -= 1\n",
    "\n",
    "    end_ind = len(sents) # exclusive\n",
    "    while end_ind > 0:\n",
    "        if (text not in ''.join(sents[:end_ind])) or (end_ind <= start_ind):\n",
    "            break\n",
    "        end_ind -= 1\n",
    "    end_ind += 1\n",
    "\n",
    "    if (start_ind < 0) or (end_ind > len(sents)):\n",
    "        print(\"\\nCan't find source sentences\\n\")\n",
    "        return None\n",
    "    \n",
    "    src_sents = ''.join(sents[max(start_ind - span, 0):end_ind])\n",
    "    return src_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subject_to_relative_clause(original_sent, clause):\n",
    "    \"\"\"Find and Prepend (with modifications) the subject of relative clause that does not contain one\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): sentence from which the clause is extracted\n",
    "        clause (str): clause for which to find subject\n",
    "    Return:\n",
    "        str: subject\n",
    "    \"\"\"\n",
    "    doc = nlp(original_sent)\n",
    "    subj = \"\"\n",
    "    clause_start_ind = original_sent.find(clause)\n",
    "    for token in doc:\n",
    "        if (len(subj)):\n",
    "            break\n",
    "        if (token.dep_ in ['relcl', 'acl']) and (token.idx >= clause_start_ind) and (token.idx < (clause_start_ind + len(clause))): # relative clause is noun modifier\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if token.head.i >= chunk.start and token.head.i < chunk.end:\n",
    "                    subj = chunk.text\n",
    "                    break\n",
    "        \n",
    "        # WORKING: relative clause is verb/adverb/adjective modifier, not sure if it's necessary tho\n",
    "        # 'cause adverbial clauses are often in relations that do not require unshortening of clause\n",
    "        if (token.dep_ in ['advcl', 'ccomp']) and (token.idx >= clause_start_ind) and (token.idx < (clause_start_ind + len(clause))): \n",
    "            print(\"\\nFound Adverbial Clause\\n\")\n",
    "            return None\n",
    "            # for chunk in doc.noun_chunks:\n",
    "            #     if token.head.i >= chunk.start and token.head.i < chunk.end:\n",
    "            #         subj = chunk.text\n",
    "            #         break\n",
    "                    \n",
    "    if not len(subj): # not found subject\n",
    "        print(\"\\nCan't find subject of\", original_sent, \"!\\n\")\n",
    "        return None         \n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.dep_ == \"nsubj\") and (token.text.lower() in RELATIVE_PRONOUNS):\n",
    "            new_clause = clause.replace(token.text, subj) # contains more nuances (where -> in + N, which -> N, who -> N)\n",
    "            new_sent = original_sent.replace(clause.strip(), new_clause,strip(), 1)\n",
    "            return new_sent, new_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add subject test run\n",
    "txt =\"For instance, on the MNLI task, the BERT_base accuracy improves by 1.0% when that trains on 1M steps (128,000 words batch size)\"\n",
    "txt_c = \"that trains on 1M steps (128,000 words batch size)\"\n",
    "add_subject_to_relative_clause(txt, txt_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_conjunction(original_sent, text):\n",
    "    \"\"\"Remove leading conjunctions from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PUNCT', 'SYM']:\n",
    "            continue\n",
    "        if \"CONJ\" not in token.pos_:\n",
    "            return (original_sent, text)\n",
    "        conj = token.text\n",
    "        break\n",
    "\n",
    "    new_text = text.replace(conj.strip(), '', 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "\n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_adverb(original_sent, text):\n",
    "    \"\"\"Remove leading adverb from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PUNCT', 'SYM']:\n",
    "            continue\n",
    "        if token.pos_ != 'ADV':\n",
    "            return (original_sent, text)\n",
    "        adv = token.text\n",
    "        break\n",
    "\n",
    "    new_text = text.replace(adv.strip(), '', 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "\n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOURSE_MARKERS = [\n",
    "  'accordingly', 'additionally', 'afterward', 'also',\n",
    "  'although', 'as a final point', 'as a result', 'assuming that', 'because', 'this is because'\n",
    "  'besides', 'but also', 'compared to', 'consequently', 'conversely', 'despite',\n",
    "  'even though', 'finally', 'first', 'firstly', 'for example', 'for instance',\n",
    "  'for the purpose of', 'furthermore', 'hence', 'however', 'if', 'importantly',\n",
    "  'in addition', 'in case', 'in conclusion', 'in contrast', 'by contrast', 'in fact',\n",
    "  'in order to', 'in other words', 'in the event that', 'in the same way',\n",
    "  'indeed', 'just as', 'lastly', 'likewise', 'moreover', 'namely',\n",
    "  'nevertheless', 'next', 'nonetheless', 'not only', 'of course', 'on condition that',\n",
    "  'on the contrary', 'on the one hand', 'on the other hand', 'otherwise', 'plus', 'previously',\n",
    "  'provided that', 'second', 'secondly', 'similarly', 'similarly to', 'since',\n",
    "  'so', 'so that', 'specifically', 'subsequently', 'such as', 'that is to say', 'that is'\n",
    "  'then', 'therefore', 'third', 'thirdly', 'thus', 'to conclude', 'to illustrate',\n",
    "  'to put it differently', 'to sum up', 'ultimately', 'undoubtedly', 'unless',\n",
    "  'while', 'with the aim of', 'yet', 'then', 'and then'\n",
    "  'as a consequence', 'as a result',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_leading_discourse_marker(original_sent, text):\n",
    "    \"\"\"Remove leading discourse markers from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    discourse_marker = None\n",
    "    min_pos = len(text)\n",
    "    for dm in DISCOURSE_MARKERS:\n",
    "        find_result = text.lower().find(dm)\n",
    "        if find_result > -1:\n",
    "            if find_result < min_pos:\n",
    "                min_pos = find_result\n",
    "                discourse_marker = dm\n",
    "            if find_result == min_pos and len(dm) > len(discourse_marker):\n",
    "                discourse_marker = dm\n",
    "    \n",
    "    # check if found marker stand at the beginning of text\n",
    "        \n",
    "    if discourse_marker is not None:\n",
    "        for i in range(0, min_pos):\n",
    "            if text[i].isalnum() or text[min_pos + len(discourse_marker)].isalnum():\n",
    "                discourse_marker = None\n",
    "                break\n",
    "            \n",
    "    if not discourse_marker:\n",
    "        return (original_sent, text)\n",
    "\n",
    "    pattern = re.compile(discourse_marker, re.IGNORECASE)\n",
    "\n",
    "    new_text = pattern.sub(\"\", text, 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "    \n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be integrated\n",
    "\n",
    "def replace_substr(text, substring, start_ind, end_ind):\n",
    "  \"\"\"Replace part of text with specified index [start_ind, end_ind) with substring\n",
    "  \"\"\"\n",
    "  try:\n",
    "    assert(len(substring)  == end_ind - start_ind)\n",
    "  except:\n",
    "    print('Text:', text, '--', sep='')\n",
    "    print('Substring:', substring, '--', sep='')\n",
    "\n",
    "  text_l = list(text)\n",
    "  text_l[start_ind:end_ind] = list(substring)\n",
    "\n",
    "  return ''.join(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_ref(index, clusters):\n",
    "    \"\"\"Check if current index is in one of the references\n",
    "    \n",
    "    Args:\n",
    "        index (int): index to check\n",
    "        clusters (list(list(tuple))): list of cluster, each cluster containing a list of tuple correponding to indices of the references\n",
    "    Return:\n",
    "        tuple (verdict, (start, end), (ref_token_start, ref_token_end)): -1 both index if not found, ref_token is token to relace\n",
    "    \"\"\"\n",
    "    for cluster in clusters:\n",
    "        for token in cluster:\n",
    "            if cluster.index(token) == 0:\n",
    "                continue\n",
    "            if index >= token[0] and index < token[1]:\n",
    "                ref_token = (cluster[0][0], cluster[0][1])\n",
    "                return True, token, ref_token\n",
    "            \n",
    "    return False, (-1, -1), (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_clause_type(clause):\n",
    "    \"\"\"Check which type of relative clause \"clause\" is: \n",
    "    - which + V + clause (present subject is sufficient for being clause)\n",
    "    - which + V + (not clause)\n",
    "\n",
    "    Args:\n",
    "        sent (str): \n",
    "        clause (str): \n",
    "    Return: 0 or 1, -1 if not relative clause expected (not start with which + V), then it could be an adverbial clause\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(clause.strip())\n",
    "\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        token = doc[i]\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" not in token.pos_: \n",
    "                return -1\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    i += 1 \n",
    "    while i < len(doc):\n",
    "        token = doc[i]\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"VERB\" not in token.pos_ and \"AUX\" not in token.pos_: \n",
    "                return -1        \n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "    \n",
    "    # if reach here, is expected relative clause type (which + V)\n",
    "    i += 1\n",
    "    t_i = i # use to this to check subject and not modify i\n",
    "    while t_i < len(doc):\n",
    "        token = doc[t_i]\n",
    "        if 'nsubj' in token.dep_:\n",
    "            return 0\n",
    "        t_i += 1\n",
    "\n",
    "    return 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be integrated\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "if 'coref_resolver' not in locals(): # prevent accidental re-run of cell\n",
    "    coref_resolver = LingMessCoref(device='cuda:0')\n",
    "\n",
    "def resolve_coreference(original_sent, text):\n",
    "    \"\"\"Perfrom coreference resolution and replace corresponding text.\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): Sentence the text was derived froms\n",
    "        text (str): Target text\n",
    "    \"\"\"\n",
    "\n",
    "    text_ind = original_sent.find(text) # starting index of text in original text\n",
    "    coref_preds = coref_resolver.predict(texts=[original_sent])\n",
    "    coref_clusters = coref_preds[0].get_clusters(as_strings=False)\n",
    "    new_sent = []\n",
    "    new_text = []\n",
    "\n",
    "    # interate string left to right while appending current char to a new list\n",
    "    # if current index in one of the token in one of the clusters, add the replacement to the list, keep the text intact, to know what index are at\n",
    "    i = 0\n",
    "\n",
    "    # if referred word is a verb, use have to notice and discard the sentence.\n",
    "    while i < len(original_sent):\n",
    "        find_result = is_in_ref(i, coref_clusters)\n",
    "        if find_result[0]:\n",
    "            token = find_result[1]\n",
    "            token_ref = find_result[2]\n",
    "            new_sent.append(original_sent[token_ref[0]:token_ref[1]])\n",
    "            if (i >= text_ind and i < text_ind + len(text)):\n",
    "                if (text_ind <= token[0] and token[1] <= text_ind + len(text)):\n",
    "                    new_text.append(original_sent[token_ref[0]:token_ref[1]])\n",
    "                else:\n",
    "                    new_text.append(original_sent[i:text_ind + len(text)])\n",
    "            i = token[1]\n",
    "        else:\n",
    "            new_sent.append(original_sent[i])\n",
    "            if i >= text_ind and i < text_ind + len(text):\n",
    "                new_text.append(original_sent[i])\n",
    "            i += 1\n",
    "\n",
    "    return ''.join(new_sent), ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 15:49:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.32 examples/s]\n",
      "05/06/2024 15:49:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CorefResult(text=\"Transformers process input sequences in parallel, ...\", clusters=[['Transformers', 'them']])]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coref test run    \n",
    "txt = \"Transformers process input sequences in parallel, making the them highly efficient for training and inference\"\n",
    "# sents = split_into_sentences(original_text)\n",
    "# src = find_source_sents(sents, txt, 5)\n",
    "# resolve_coreference(src, txt)\n",
    "coref_preds = coref_resolver.predict(texts=[txt])\n",
    "coref_clusters = coref_preds[0].get_clusters(as_strings=False)\n",
    "coref_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(original_text, text, retain_dm=False, retain_conj=False, retain_adv=False, add_subject_to_rel_clause=False):\n",
    "    \"\"\"Tranform raw text into a full clause to be fed to the question generation pipeline\n",
    "\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "    Return:\n",
    "        (str): full clause from text\n",
    "    \"\"\"\n",
    "    sents = split_into_sentences(original_text)\n",
    "\n",
    "    # coreferen ce resolution\n",
    "    src_3_sents = find_source_sents(sents, text, 3)\n",
    "    src_3_sents, text = resolve_coreference(src_3_sents, text)\n",
    "    src_sent = find_source_sents(split_into_sentences(src_3_sents), text, 0)\n",
    "\n",
    "    # removal of irrelavent components\n",
    "    if not retain_dm:\n",
    "        src_sent, text = remove_leading_discourse_marker(src_sent, text)    \n",
    "    if not retain_conj:\n",
    "        src_sent, text = remove_leading_conjunction(src_sent, text)\n",
    "    if not retain_adv: \n",
    "        src_sent, text = remove_leading_adverb(src_sent, text)\n",
    "    src_sent = remove_leading_special_chars(src_sent)\n",
    "    text = remove_leading_special_chars(text)\n",
    "    # src_3_sents = remove_ending_special_chars(src_3_sents)\n",
    "    # text = remove_ending_special_chars(text)\n",
    "\n",
    "    # handle single relative clause\n",
    "    if is_one_clause(text, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(text)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            if cl_type == 2:\n",
    "                print(\"\\nText here: \", src_sent, text)\n",
    "                src_sent, text = unshorten_relative_clause(src_sent, text)\n",
    "            if add_subject_to_rel_clause:\n",
    "                # WORKING: temporary solution before fixing add_subject_to_relative_clause()\n",
    "                if add_subject_to_relative_clause(src_sent, text):\n",
    "                    src_sent, text = add_subject_to_relative_clause(src_sent, text)\n",
    "\n",
    "    return src_sent, text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Training: Transformer models are trained using supervised learning',\n",
       " 'using which is supervised learning.')"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unshorten_relative_clause(\"Training: Transformer models are trained using supervised learning\", \"using supervised learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('She was talking to me, a person that is tackled.', ' that is tackled.')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unshorten_relative_clause(\"She was talking to me, a person tackled.\", \" a person tackled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 15:40:36 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.82 examples/s]\n",
      "05/06/2024 15:40:36 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a person tackled.'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline(\"She was talking to me, a person tackled.\", \" a person tackled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORKING: \n",
    "- Handle non-clause (maybe just keep all, discard \"which + V\" maybe)\n",
    "- Handle missing information (two EDUs still don't make a sentence) -> maybe segment longer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file\n",
    "output_path = './output.txt'\n",
    "sents = split_into_sentences(original_text) # text split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cause_question_type_0(nucleus, satellite): # WORKING: nucleus cause satellite (but now model interpret both directions, needs fixing)\n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting:exactly the same as type_1\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def cause_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which caused the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def cause_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(satellite):\n",
    "        new_sate = move_aux_to_beginning(satellite)\n",
    "    else: \n",
    "        new_sate = choose_and_replace_aux_for_verb(satellite)\n",
    "    question = \"Why \" + new_sate.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "                                             \n",
    "def generate_cause_question(nucleus_pair, satellite_pair):\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "    if is_one_clause(satellite, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(satellite)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            rel_type = check_relative_clause_type(satellite)\n",
    "            if rel_type == 0:\n",
    "                return cause_question_type_0(nucleus, satellite)\n",
    "            elif rel_type == 1:\n",
    "                return cause_question_type_1(nucleus, satellite)\n",
    "    else:\n",
    "        return (\"\", \"\")\n",
    "    return cause_question_type_2(nucleus, satellite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' These networks apply non-linear transformations to the token representations, allowing the model to capture complex patterns and relationships in the data.'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = split_into_sentences(original_text)\n",
    "find_source_sents(sents, \"These networks apply non-linear transformations to the token representations,\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:06:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 91.07 examples/s]\n",
      "05/06/2024 16:06:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "05/06/2024 16:06:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 140.51 examples/s]\n",
      "05/06/2024 16:06:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n",
      "05/06/2024 16:06:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 95.66 examples/s]\n",
      "05/06/2024 16:06:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "05/06/2024 16:06:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 133.53 examples/s]\n",
      "05/06/2024 16:06:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n",
      "05/06/2024 16:06:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 99.21 examples/s]\n",
      "05/06/2024 16:06:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
      "05/06/2024 16:06:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 137.06 examples/s]\n",
      "05/06/2024 16:06:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s]\n",
      "05/06/2024 16:06:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 97.66 examples/s]\n",
      "05/06/2024 16:06:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "05/06/2024 16:06:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.68 examples/s]\n",
      "05/06/2024 16:06:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "05/06/2024 16:06:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 106.47 examples/s]\n",
      "05/06/2024 16:06:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "05/06/2024 16:06:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 125.04 examples/s]\n",
      "05/06/2024 16:06:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "05/06/2024 16:06:53 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.68 examples/s]\n",
      "05/06/2024 16:06:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "05/06/2024 16:06:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 106.43 examples/s]\n",
      "05/06/2024 16:06:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n"
     ]
    }
   ],
   "source": [
    "rel = 'Cause'\n",
    "for trip in df[df['new_relation'] == rel].iterrows():\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        f.write(\"Original nucleus: \" + nucleus + '\\n')\n",
    "        f.write(\"Original satellite: \" + satellite + '\\n')\n",
    "\n",
    "        nucleus_pair = preprocessing_pipeline(original_text, nucleus)\n",
    "        satellite_pair = preprocessing_pipeline(original_text, satellite)\n",
    "        \n",
    "        if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "            f.write(\"Can't process nucleus and satellite!\\n\")\n",
    "            continue\n",
    "        \n",
    "        ques, ans = generate_cause_question(nucleus_pair, satellite_pair) # WORKING: order of cause and serveral other relations are not consistant, fix dataset\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')\n",
    "\n",
    "        ques, ans = generate_cause_question(satellite_pair, nucleus_pair) # WORKING: order of cause and serveral other relations are not consistant, fix dataset\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast relation: ask what's different between two subjects.\n",
    "# retain discourse markers\n",
    "    \n",
    "def make_contrast_question_with_two_clauses(nucleus, satellite):\n",
    "    n_subj = get_subj(nucleus)\n",
    "    s_subj = get_subj(satellite)\n",
    "   \n",
    "    if (n_subj is None) or (s_subj is None):\n",
    "        print(\"\\nCan't find subject!\\n\")\n",
    "        return (\"\", \"\")\n",
    "     \n",
    "    if n_subj.strip().lower() == s_subj.strip().lower():\n",
    "        print(\"\\nSame subjects for nucleus and satellite!\\n\")\n",
    "        return (\"\", \"\")\n",
    "    \n",
    "    question = \"What is the difference between \" + n_subj + \" and \" + s_subj + '?'\n",
    "    answer = nucleus.strip() + ' ' + satellite.strip()\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_contrast_question(nucleus_pair, satellite_pair):\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "    \n",
    "    if is_one_clause(nucleus, count_relative_clause=False) and is_one_clause(satellite, count_relative_clause=False):\n",
    "        return make_contrast_question_with_two_clauses(nucleus, satellite)\n",
    "    return (\"\", \"\") # more than 1 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:08:48 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 109.63 examples/s]\n",
      "05/06/2024 16:08:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "05/06/2024 16:08:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 31.92 examples/s]\n",
      "05/06/2024 16:08:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "05/06/2024 16:08:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 96.92 examples/s]\n",
      "05/06/2024 16:08:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "05/06/2024 16:08:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 96.24 examples/s]\n",
      "05/06/2024 16:08:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# test run for Contrast\n",
    "rel = 'Contrast'\n",
    "for trip in df[df['new_relation'] == rel].iterrows():\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        f.write(\"\\nOriginal nucleus: \" + trip[1]['nucleus'] + '\\n')\n",
    "        f.write(\"Original satellite: \" + trip[1]['satellite'] + '\\n')\n",
    "\n",
    "        nucleus_pair = preprocessing_pipeline(original_text, nucleus, retain_dm=True, add_subject_to_rel_clause=True)\n",
    "        satellite_pair = preprocessing_pipeline(original_text, satellite, retain_dm=True, retain_adv=True, retain_conj=True, add_subject_to_rel_clause=True)\n",
    "        \n",
    "        if nucleus[1] is None or satellite[1] is None:\n",
    "            f.write(\"Can't process nucleus and satellite!\\n\")\n",
    "            continue\n",
    "\n",
    "        ques, ans = generate_contrast_question(nucleus_pair, satellite_pair)\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            continue\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition relation\n",
    "\n",
    "def condition_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting:exactly the same as type_1\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What condition\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def condition_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which conditiond the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What condition\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def condition_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(satellite):\n",
    "        new_sate = move_aux_to_beginning(satellite)\n",
    "    else: \n",
    "        new_sate = choose_and_replace_aux_for_verb(satellite)\n",
    "    question = \"In what condition \" + new_sate.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "                                             \n",
    "def generate_condition_question(nucleus_pair, satellite_pair):\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "    \n",
    "    if is_one_clause(satellite, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(satellite)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            rel_type = check_relative_clause_type(satellite)\n",
    "            if rel_type == 0:\n",
    "                return condition_question_type_0(nucleus, satellite)\n",
    "            elif rel_type == 1:\n",
    "                return condition_question_type_1(nucleus, satellite)\n",
    "        else: # 1 clause, not relative clause \n",
    "            return condition_question_type_2(nucleus, satellite)\n",
    "    else:\n",
    "        return (\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:09:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 126.91 examples/s]\n",
      "05/06/2024 16:09:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "05/06/2024 16:09:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 44.64 examples/s]\n",
      "05/06/2024 16:09:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# for Condition relation\n",
    "\n",
    "rel = 'Condition'\n",
    "for trip in df[df['new_relation'] == rel].iterrows():\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "        nucleus = trip[1]['nucleus']\n",
    "        satellite = trip[1]['satellite']\n",
    "\n",
    "        f.write(\"Original nucleus: \" + nucleus + '\\n')\n",
    "        f.write(\"Original satellite: \" + satellite + '\\n')\n",
    "\n",
    "        nucleus_pair = preprocessing_pipeline(original_text, nucleus)\n",
    "        satellite_pair = preprocessing_pipeline(original_text, satellite)\n",
    "        \n",
    "        if nucleus[1] is None or satellite[1] is None:\n",
    "            f.write(\"Can't process nucleus and satellite!\\n\")\n",
    "            continue\n",
    "        \n",
    "        ques, ans = generate_condition_question(nucleus_pair, satellite_pair)\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')\n",
    "\n",
    "        ques, ans = generate_condition_question(satellite_pair, nucleus_pair)\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for enablement's purpose: \n",
    "# clause -> reverse\n",
    "# preposition/adposition -> keep \n",
    "# relative clause (both types) -> keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enablement relation\n",
    "# difference to manner-means: enablement encapsulate manner-means, as all means can \"enable\" the goal,\n",
    "# but enablement also contains situational aid, an event lead (may not intentionally) to another event\n",
    "    \n",
    "# enablement relation\n",
    "\n",
    "def enablement_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting: exactly the same as type_1\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def enablement_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which enablementd the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def enablement_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(nucleus):\n",
    "        new_nuc = move_aux_to_beginning(nucleus)\n",
    "    else: \n",
    "        new_nuc = choose_and_replace_aux_for_verb(nucleus)\n",
    "    question = \"How \" + new_nuc.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def enablement_question_type_3(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 2: satellite (result) is: relative clause but not start with relative pronoun (most probably adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    question = \"What can be done \" + nucleus.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_enablement_question(nucleus_pair, satellite_pair):\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    if is_one_clause(nucleus, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(nucleus)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            rel_type = check_relative_clause_type(nucleus)\n",
    "            if rel_type == 0:\n",
    "                return enablement_question_type_0(nucleus, satellite)\n",
    "            elif rel_type == 1:\n",
    "                return enablement_question_type_1(nucleus, satellite)\n",
    "        else:\n",
    "            if is_dependent_clause(nucleus_pair[0], nucleus):\n",
    "                return enablement_question_type_3(nucleus, satellite)\n",
    "            return enablement_question_type_2(nucleus, satellite)\n",
    "    return (\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 93.56 examples/s]\n",
      "05/06/2024 16:41:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "05/06/2024 16:41:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 91.71 examples/s]\n",
      "05/06/2024 16:41:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 27\n",
      "In sequence-to-sequence tasks like neural machine translation, a separate decoder module can be added on top of the encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:50 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 33\n",
      "to generate the output sequence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.62 examples/s]\n",
      "05/06/2024 16:41:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "05/06/2024 16:41:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.66 examples/s]\n",
      "05/06/2024 16:41:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14\n",
      "Layer normalization and residual connections: The model uses layer normalization and residual connections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:52 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 21\n",
      "to stabilize and speed up training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 55.39 examples/s]\n",
      "05/06/2024 16:41:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
      "05/06/2024 16:41:52 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 77.86 examples/s]\n",
      "05/06/2024 16:41:52 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 11\n",
      "are used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:53 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 22\n",
      "to calculate attention weights in the self-attention mechanism.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 116.34 examples/s]\n",
      "05/06/2024 16:41:53 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "05/06/2024 16:41:54 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 104.39 examples/s]\n",
      "05/06/2024 16:41:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "05/06/2024 16:41:54 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 24\n",
      "to capture different types of relationships between tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 115.81 examples/s]\n",
      "05/06/2024 16:41:54 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "05/06/2024 16:41:55 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 112.22 examples/s]\n",
      "05/06/2024 16:41:55 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 11\n",
      "you'd need to take\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:56 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 19\n",
      "to accomplish convert with a transformer model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.95 examples/s]\n",
      "05/06/2024 16:41:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "05/06/2024 16:41:56 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 102.05 examples/s]\n",
      "05/06/2024 16:41:56 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6\n",
      "that you need\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 16:41:57 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 14\n",
      "to convert an English sentence into French.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 112.90 examples/s]\n",
      "05/06/2024 16:41:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "05/06/2024 16:41:57 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 95.62 examples/s]\n",
      "05/06/2024 16:41:57 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 47\n",
      "the model can predict words which are likely to be used in sequence.\n"
     ]
    }
   ],
   "source": [
    "# test run for Enablement\n",
    "rel = 'Enablement'\n",
    "\n",
    "with open(output_path, 'a') as f:\n",
    "    for trip in df[df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus']\n",
    "        satellite = trip[1]['satellite']\n",
    "\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "        f.write(\"\\nOriginal nucleus: \" + nucleus + '\\n')\n",
    "        f.write(\"Original satellite: \" + satellite + '\\n')\n",
    "\n",
    "        nucleus_pair = preprocessing_pipeline(original_text, nucleus, retain_dm=True, retain_conj=True, retain_adv=True)\n",
    "        satellite_pair = preprocessing_pipeline(original_text, satellite)\n",
    "       \n",
    "        if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "            f.write(\"Can't process nucleus and satellite!\\n\")\n",
    "            continue\n",
    "\n",
    "        ques, ans = generate_enablement_question(nucleus_pair, satellite_pair)\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')\n",
    "\n",
    "        ques, ans = generate_enablement_question(satellite_pair, nucleus_pair)\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manner-Means relation\n",
    "    \n",
    "# manner-means relation\n",
    "# difference to manner-means: manner-means encapsulate manner-means, as all means can \"enable\" the goal,\n",
    "# but manner-means also contains situational aid, an event lead (may not intentionally) to another event\n",
    "    \n",
    "# manner-means relation\n",
    "\n",
    "def means_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting: exactly the same as type_1\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What method \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def means_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which caused the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What method \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def means_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(nucleus):\n",
    "        new_nuc = move_aux_to_beginning(nucleus)\n",
    "    else: \n",
    "        new_nuc = choose_and_replace_aux_for_verb(nucleus)\n",
    "    question = \"By what method \" + new_nuc.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def means_question_type_3(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 2: satellite (result) is: relative clause but not start with relative pronoun (most probably adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    question = \"What strategy can be employed \" + nucleus.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_means_question(nucleus_pair, satellite_pair):\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "    \n",
    "    if is_one_clause(nucleus, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(nucleus)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            rel_type = check_relative_clause_type(nucleus)\n",
    "            if rel_type == 0:\n",
    "                return means_question_type_0(nucleus, satellite)\n",
    "            elif rel_type == 1:\n",
    "                return means_question_type_1(nucleus, satellite)\n",
    "        else:\n",
    "            if is_dependent_clause(nucleus_pair[0], nucleus):\n",
    "                return means_question_type_3(nucleus, satellite)\n",
    "            return means_question_type_2(nucleus, satellite)\n",
    "    return (\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 22:52:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 113.78 examples/s]\n",
      "05/06/2024 22:52:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "05/06/2024 22:52:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 118.12 examples/s]\n",
      "05/06/2024 22:52:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6\n",
      "Training: Transformer models are trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 22:52:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 36\n",
      "using supervised learning, where Transformer models learn to minimize a loss function that quantifies the difference between Transformer models predictions and the ground truth for the given task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 87.50 examples/s]\n",
      "05/06/2024 22:52:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text here:  Training: Transformer models are trained using supervised learning, where Transformer models learn to minimize a loss function that quantifies the difference between Transformer models predictions and the ground truth for the given task. using supervised learning,\n",
      "learning\n",
      "\n",
      "Found Adverbial Clause\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[683], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal nucleus: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m nucleus \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal satellite: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m satellite \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m nucleus_pair \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnucleus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_dm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_conj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_adv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_subject_to_rel_clause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m satellite_pair \u001b[38;5;241m=\u001b[39m preprocessing_pipeline(original_text, satellite)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nucleus_pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m satellite_pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[682], line 36\u001b[0m, in \u001b[0;36mpreprocessing_pipeline\u001b[0;34m(original_text, text, retain_dm, retain_conj, retain_adv, add_subject_to_rel_clause)\u001b[0m\n\u001b[1;32m     34\u001b[0m             src_sent, text \u001b[38;5;241m=\u001b[39m unshorten_relative_clause(src_sent, text)\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m add_subject_to_rel_clause:\n\u001b[0;32m---> 36\u001b[0m             src_sent, text \u001b[38;5;241m=\u001b[39m add_subject_to_relative_clause(src_sent, text)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src_sent, text\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# test run for Manner-Means\n",
    "rel = 'Manner-Means'\n",
    "with open(output_path, 'a') as f:\n",
    "    for trip in df[df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus']\n",
    "        satellite = trip[1]['satellite']\n",
    "\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "        f.write(\"\\nOriginal nucleus: \" + nucleus + '\\n')\n",
    "        f.write(\"Original satellite: \" + satellite + '\\n')\n",
    "\n",
    "        nucleus_pair = preprocessing_pipeline(original_text, nucleus, retain_dm=True, retain_conj=True, retain_adv=True, add_subject_to_rel_clause=True)\n",
    "        satellite_pair = preprocessing_pipeline(original_text, satellite)\n",
    "\n",
    "        if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "            f.write(\"Can't process nucleus and satellite!\\n\")\n",
    "            continue\n",
    "\n",
    "        ques, ans = generate_means_question(nucleus_pair, satellite_pair) \n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')\n",
    "\n",
    "        ques, ans = generate_means_question(satellite_pair, nucleus_pair) \n",
    "        if not ques.strip() or not ans.strip():\n",
    "            f.write(\"Can't generate questions!\\n\")\n",
    "            \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can't find subject of Attention allows each word to attend to every other word in the sequence in parallel, that weighs every other word in the sequence importance for the current token !\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(add_subject_to_relative_clause(\"Attention allows each word to attend to every other word in the sequence in parallel, that weighs every other word in the sequence importance for the current token\", \"that weighs every other word in the sequence importance for the current token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING: for questions based on BACKGROUND relation, haven't adapted to new preprocessing pipeline\n",
    "\n",
    "def make_background_question_with_two_clauses(nucleus, satellite): \n",
    "    question = \"Under what circumstance that \" + nucleus.strip() + '?'\n",
    "    answer = satellite\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_background_question(original_text, nucleus, satellite):\n",
    "    nucleus = preprocessing_pipeline(original_text, nucleus, retain_dm=False, retain_conj=True, retain_adv=True) # NOTE: may not want to retain dm here, 'cause need only the dm between nuc and sate\n",
    "    satellite = preprocessing_pipeline(original_text, satellite, retain_dm=False, retain_conj=True, retain_adv=True)\n",
    "    \n",
    "    if nucleus is None or satellite is None:\n",
    "        print(\"\\nCan't process nucleus and satellite!\\n\")\n",
    "        return (\"\", \"\")\n",
    "    \n",
    "    if is_one_clause(nucleus) and is_one_clause(satellite):\n",
    "        return make_background_question_with_two_clauses(nucleus, satellite)\n",
    "    else:\n",
    "        return make_background_question_with_two_clauses(nucleus, satellite)\n",
    "        return (\"\", \"\") # other cases, not case with two clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run for Background\n",
    "rel = 'Background'\n",
    "for trip in df[df['new_relation'] == rel].iterrows():\n",
    "    with open(output_path, 'a') as f:\n",
    "        f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "\n",
    "        f.write(\"\\nOriginal nucleus: \" + trip[1]['nucleus'] + '\\n')\n",
    "        f.write(\"Original satellite: \" + trip[1]['satellite'] + '\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        ques, ans = generate_background_question(original_text, trip[1]['nucleus'], trip[1]['satellite'])\n",
    "        if not ques.strip() or not ans.strip():\n",
    "            continue\n",
    "        \n",
    "        f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "        f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Question Generator (for random testings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_one_clause(\"This characteristic allows the model to learn the context of a word \", count_relative_clause=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2024 17:37:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.72 examples/s]\n",
      "05/06/2024 17:37:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "05/06/2024 17:37:58 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 122.66 examples/s]\n",
      "05/06/2024 17:37:58 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each layer processes the output of the previous layer,  that refines the representations. that refines the representations.\n",
      "\n",
      "Question: What method  refines the representations.?\n",
      "Answer: Each layer processes the output of the previous layer,.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# miscellaneous test run\n",
    "nuc = \"\"\"Each layer processes the output of the previous layer, \"\"\"\n",
    "sate = \"gradually refining the representations.\" \n",
    "org = nuc + sate\n",
    "\n",
    "nucleus_pair = preprocessing_pipeline(original_text, nuc, retain_dm=True, retain_conj=True, retain_adv=True)\n",
    "satellite_pair = preprocessing_pipeline(original_text, sate)\n",
    "\n",
    "if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "    print(\"Can't process nucleus and satellite!\\n\")\n",
    "\n",
    "ques, ans = generate_means_question(satellite_pair, nucleus_pair)\n",
    "if not ques.strip() or not ans.strip():\n",
    "    print(\"Can't generate questions!\\n\")\n",
    "        \n",
    "print(\"\\nQuestion: \" + ques)\n",
    "print(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found Adverbial Clause\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_subject_to_relative_clause(org, sate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 21\n",
      "to stabilize and speed up training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dependent_clause(satellite_pair[0], \"to stabilize and speed up training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT_large, with 345 million parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT_base, which uses the same architecture with -only- 110 million parameters.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = split_into_sentences(original_text)\n",
    "src_sent = find_source_sents(sents, \"is the largest model of its kind. It is demonstrably superior on\", span=0)\n",
    "src_sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
